{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 1: IMPORTS AND LIBRARIES\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Face GAN - Deep Convolutional GAN for Face Generation\n",
    "----------------------------------------------------\n",
    "Import required libraries and modules for dataset processing,\n",
    "neural network creation, training, and visualization.\n",
    "\"\"\"\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter progress bars\n",
    "\n",
    "# Try importing optional libraries\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    TENSORBOARD_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TENSORBOARD_AVAILABLE = False\n",
    "    print(\"TensorBoard not available. Install with: pip install tensorboard\")\n",
    "\n",
    "# For resource monitoring\n",
    "try:\n",
    "    import psutil\n",
    "    import GPUtil\n",
    "    RESOURCE_MONITORING = True\n",
    "except ImportError:\n",
    "    RESOURCE_MONITORING = False\n",
    "    print(\"Resource monitoring unavailable. Install with: pip install psutil gputil\")\n",
    "\n",
    "# Set display settings for the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset paths:\n",
      "- CelebDF: c:\\Users\\vinay\\Documents\\mnist\\faces\\Real\\Celeb_V2\\Train\\real\n",
      "- FaceForensics++: c:\\Users\\vinay\\Documents\\mnist\\faces\\Real\\FaceForensics++\\original_sequences\\youtube\\c23\\frames\n",
      "Processed data will be saved to: c:\\Users\\vinay\\Documents\\mnist\\processed_faces\n",
      "Output will be saved to: c:\\Users\\vinay\\Documents\\mnist\\output\n",
      "Target image size: 128x128\n",
      "Batch size: 32\n",
      "Training epochs: 10\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# CELL 2: CONFIGURATION SETTINGS\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Configuration Settings\n",
    "---------------------\n",
    "Define paths, hyperparameters, and training settings.\n",
    "Modify these values to adapt the model to your specific requirements.\n",
    "\"\"\"\n",
    "# Paths\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "CELEBDF_PATH = os.path.join(BASE_DIR, \"faces\\\\Real\\\\Celeb_V2\\\\Train\\\\real\")\n",
    "FF_PATH = os.path.join(BASE_DIR, \"faces\\\\Real\\\\FaceForensics++\\\\original_sequences\\\\youtube\\\\c23\\\\frames\")\n",
    "PROCESSED_PATH = os.path.join(BASE_DIR, \"processed_faces\")\n",
    "OUTPUT_PATH = os.path.join(BASE_DIR, \"output\")\n",
    "CHECKPOINT_DIR = os.path.join(OUTPUT_PATH, \"checkpoints\")\n",
    "LOG_DIR = os.path.join(OUTPUT_PATH, \"logs\")\n",
    "\n",
    "# Dataset processing settings\n",
    "PROCESS_DATASETS = True  # Set to False to skip dataset processing if already processed\n",
    "CELEBDF_MAX_PER_FACE = None  # Max images per face for CelebDF\n",
    "FF_MAX_PER_FACE = None  # Max images per face for FaceForensics++\n",
    "FF_MAX_FACES = None  # Maximum number of different faces from FaceForensics++\n",
    "TARGET_SIZE = 128  # Size to resize images to\n",
    "\n",
    "# Model hyperparameters\n",
    "CUDA = True  # Use CUDA (will be auto-detected later)\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_CHANNEL = 3  # RGB images\n",
    "Z_DIM = 100  # Latent vector dimension\n",
    "G_HIDDEN = 64  # Generator hidden dimension\n",
    "D_HIDDEN = 64  # Discriminator hidden dimension\n",
    "X_DIM = 128  # Target image size\n",
    "EPOCH_NUM = 10  # Number of training epochs\n",
    "REAL_LABEL = 1\n",
    "FAKE_LABEL = 0\n",
    "lr = 2e-4  # Learning rate\n",
    "seed = 1  # Random seed for reproducibility\n",
    "\n",
    "# Training control settings\n",
    "CHECKPOINT_FREQ = 5  # Save checkpoints every N epochs\n",
    "CHECKPOINT_SAMPLES = 1000  # Generate image samples every N iterations\n",
    "RESUME_TRAINING = True  # Try to resume from checkpoint if available\n",
    "EARLY_STOPPING_PATIENCE = 5  # Early stopping after N epochs without improvement\n",
    "EARLY_STOPPING_THRESHOLD = 0.01  # Minimum improvement to reset patience counter\n",
    "RESOURCE_CHECK_FREQ = 50  # Check system resources every N batches\n",
    "MAX_TRAINING_TIME = None  # Max training time in hours (None for no limit)\n",
    "ENABLE_TENSORBOARD = TENSORBOARD_AVAILABLE  # Enable TensorBoard logging\n",
    "EXPERIMENT_NAME = f\"face_gan_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"  # Unique name for this run\n",
    "\n",
    "# Print some key configuration values\n",
    "print(f\"Dataset paths:\\n- CelebDF: {CELEBDF_PATH}\\n- FaceForensics++: {FF_PATH}\")\n",
    "print(f\"Processed data will be saved to: {PROCESSED_PATH}\")\n",
    "print(f\"Output will be saved to: {OUTPUT_PATH}\")\n",
    "print(f\"Target image size: {TARGET_SIZE}x{TARGET_SIZE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training epochs: {EPOCH_NUM}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output directories:\n",
      " - CelebDF: c:\\Users\\vinay\\Documents\\mnist\\processed_faces\\celebdf\n",
      " - FaceForensics++: c:\\Users\\vinay\\Documents\\mnist\\processed_faces\\faceforensics\n",
      " - Combined: c:\\Users\\vinay\\Documents\\mnist\\processed_faces\\combined\n",
      " - Results: c:\\Users\\vinay\\Documents\\mnist\\output\n",
      " - Checkpoints: c:\\Users\\vinay\\Documents\\mnist\\output\\checkpoints\n",
      " - Logs: c:\\Users\\vinay\\Documents\\mnist\\output\\logs\n",
      "CelebDF dataset: Found at c:\\Users\\vinay\\Documents\\mnist\\faces\\Real\\Celeb_V2\\Train\\real\n",
      "FaceForensics++ dataset: Found at c:\\Users\\vinay\\Documents\\mnist\\faces\\Real\\FaceForensics++\\original_sequences\\youtube\\c23\\frames\n",
      "Configuration saved to c:\\Users\\vinay\\Documents\\mnist\\output\\face_gan_20250304_161130_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\vinay\\\\Documents\\\\mnist\\\\output\\\\face_gan_20250304_161130_config.json'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################################################\n",
    "# CELL 3: HELPER FUNCTIONS\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Helper Functions\n",
    "--------------\n",
    "Create output directories, verify dataset paths,\n",
    "save configuration, and monitor system resources.\n",
    "\"\"\"\n",
    "def create_output_directories():\n",
    "    \"\"\"Create necessary output directories\"\"\"\n",
    "    os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    os.makedirs(LOG_DIR, exist_ok=True)\n",
    "    \n",
    "    # Create separate directories for each dataset\n",
    "    celebdf_dir = os.path.join(PROCESSED_PATH, \"celebdf\")\n",
    "    ff_dir = os.path.join(PROCESSED_PATH, \"faceforensics\")\n",
    "    combined_dir = os.path.join(PROCESSED_PATH, \"combined\")\n",
    "    \n",
    "    os.makedirs(celebdf_dir, exist_ok=True)\n",
    "    os.makedirs(ff_dir, exist_ok=True)\n",
    "    os.makedirs(combined_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Created output directories:\")\n",
    "    print(f\" - CelebDF: {celebdf_dir}\")\n",
    "    print(f\" - FaceForensics++: {ff_dir}\")\n",
    "    print(f\" - Combined: {combined_dir}\")\n",
    "    print(f\" - Results: {OUTPUT_PATH}\")\n",
    "    print(f\" - Checkpoints: {CHECKPOINT_DIR}\")\n",
    "    print(f\" - Logs: {LOG_DIR}\")\n",
    "    \n",
    "    return celebdf_dir, ff_dir, combined_dir\n",
    "\n",
    "def verify_dataset_paths():\n",
    "    \"\"\"Verify that dataset paths exist\"\"\"\n",
    "    celebdf_exists = os.path.exists(CELEBDF_PATH)\n",
    "    ff_exists = os.path.exists(FF_PATH)\n",
    "    \n",
    "    print(f\"CelebDF dataset: {'Found' if celebdf_exists else 'Not found'} at {CELEBDF_PATH}\")\n",
    "    print(f\"FaceForensics++ dataset: {'Found' if ff_exists else 'Not found'} at {FF_PATH}\")\n",
    "    \n",
    "    return celebdf_exists, ff_exists\n",
    "\n",
    "def save_config():\n",
    "    \"\"\"Save current configuration as a JSON file\"\"\"\n",
    "    config = {\n",
    "        'CELEBDF_PATH': CELEBDF_PATH,\n",
    "        'FF_PATH': FF_PATH,\n",
    "        'PROCESSED_PATH': PROCESSED_PATH,\n",
    "        'OUTPUT_PATH': OUTPUT_PATH,\n",
    "        'CHECKPOINT_DIR': CHECKPOINT_DIR,\n",
    "        'PROCESS_DATASETS': PROCESS_DATASETS,\n",
    "        'CELEBDF_MAX_PER_FACE': CELEBDF_MAX_PER_FACE,\n",
    "        'FF_MAX_PER_FACE': FF_MAX_PER_FACE,\n",
    "        'FF_MAX_FACES': FF_MAX_FACES,\n",
    "        'TARGET_SIZE': TARGET_SIZE,\n",
    "        'BATCH_SIZE': BATCH_SIZE,\n",
    "        'IMAGE_CHANNEL': IMAGE_CHANNEL,\n",
    "        'Z_DIM': Z_DIM,\n",
    "        'G_HIDDEN': G_HIDDEN,\n",
    "        'D_HIDDEN': D_HIDDEN,\n",
    "        'X_DIM': X_DIM,\n",
    "        'EPOCH_NUM': EPOCH_NUM,\n",
    "        'lr': lr,\n",
    "        'seed': seed,\n",
    "        'EXPERIMENT_NAME': EXPERIMENT_NAME,\n",
    "        'timestamp': datetime.datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    config_path = os.path.join(OUTPUT_PATH, f\"{EXPERIMENT_NAME}_config.json\")\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    print(f\"Configuration saved to {config_path}\")\n",
    "    return config_path\n",
    "\n",
    "def monitor_resources():\n",
    "    \"\"\"Monitor system resources and return a report\"\"\"\n",
    "    if not RESOURCE_MONITORING:\n",
    "        return \"Resource monitoring unavailable. Install psutil and gputil.\"\n",
    "    \n",
    "    # CPU info\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    memory = psutil.virtual_memory()\n",
    "    memory_percent = memory.percent\n",
    "    \n",
    "    # GPU info\n",
    "    gpu_info = \"No GPU available\"\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            if gpus:\n",
    "                gpu = gpus[0]  # Get the first GPU\n",
    "                gpu_name = gpu.name\n",
    "                gpu_load = f\"{gpu.load * 100:.1f}%\"\n",
    "                gpu_mem_used = f\"{gpu.memoryUsed:.0f}MB\"\n",
    "                gpu_mem_total = f\"{gpu.memoryTotal:.0f}MB\"\n",
    "                gpu_mem_percent = f\"{(gpu.memoryUsed / gpu.memoryTotal) * 100:.1f}%\"\n",
    "                gpu_temp = f\"{gpu.temperature}Â°C\"\n",
    "                gpu_info = f\"{gpu_name}: {gpu_load} load, {gpu_mem_used}/{gpu_mem_total} ({gpu_mem_percent}), {gpu_temp}\"\n",
    "        except Exception as e:\n",
    "            gpu_info = f\"Error getting GPU info: {e}\"\n",
    "    \n",
    "    return {\n",
    "        \"cpu_percent\": cpu_percent,\n",
    "        \"memory_percent\": memory_percent,\n",
    "        \"gpu_info\": gpu_info,\n",
    "        \"timestamp\": datetime.datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "# Create directories and verify paths\n",
    "celebdf_dir, ff_dir, combined_dir = create_output_directories()\n",
    "celebdf_exists, ff_exists = verify_dataset_paths()\n",
    "save_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80576 total images in c:\\Users\\vinay\\Documents\\mnist\\faces\\Real\\Celeb_V2\\Train\\real\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d685c600d55447bb128f6856afa604c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing CelebDF images:   0%|          | 0/80576 [00:00<?, ?img/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 80576 images from CelebDF to c:\\Users\\vinay\\Documents\\mnist\\processed_faces\\celebdf\n",
      "Processed 80576 CelebDF images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###############################################################################\n",
    "# CELL 4: CELEBDF DATASET PROCESSING\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "CelebDF Dataset Processing\n",
    "------------------------\n",
    "Process the CelebDF dataset by extracting faces, resizing them,\n",
    "and saving them to a standardized format.\n",
    "\"\"\"\n",
    "def process_celebdf_dataset(source_dir, target_dir, target_size=(128, 128), max_images=None):\n",
    "    \"\"\"Process CelebDF dataset where all images are in a single folder\"\"\"\n",
    "    source_path = Path(source_dir)\n",
    "    target_path = Path(target_dir)\n",
    "    \n",
    "    if not source_path.exists():\n",
    "        print(f\"Source directory {source_path} does not exist.\")\n",
    "        return 0\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png']\n",
    "    image_files = []\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(source_path.glob(f\"*{ext}\"))\n",
    "        image_files.extend(source_path.glob(f\"*{ext.upper()}\"))\n",
    "    \n",
    "    # If maximum image limit is set, randomly sample\n",
    "    if max_images and len(image_files) > max_images:\n",
    "        random.seed(1)  # For reproducible results\n",
    "        image_files = random.sample(image_files, max_images)\n",
    "    \n",
    "    print(f\"Found {len(image_files)} total images in {source_path}\")\n",
    "    \n",
    "    # Process all images directly with progress bar\n",
    "    count = 0\n",
    "    with tqdm(total=len(image_files), desc=\"Processing CelebDF images\", unit=\"img\") as pbar:\n",
    "        for img_path in image_files:\n",
    "            try:\n",
    "                # Open and resize image\n",
    "                img = Image.open(img_path)\n",
    "                img = img.resize(target_size, Image.LANCZOS)\n",
    "                \n",
    "                # Save to target directory\n",
    "                target_file = target_path / f\"celebdf_{count:06d}{img_path.suffix}\"\n",
    "                img.save(target_file)\n",
    "                count += 1\n",
    "                pbar.update(1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing {img_path.name}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully processed {count} images from CelebDF to {target_path}\")\n",
    "    return count\n",
    "\n",
    "# Only run if PROCESS_DATASETS is True and CelebDF dataset exists\n",
    "if PROCESS_DATASETS and celebdf_exists:\n",
    "    count = process_celebdf_dataset(\n",
    "        CELEBDF_PATH, \n",
    "        celebdf_dir, \n",
    "        target_size=(TARGET_SIZE, TARGET_SIZE),\n",
    "        max_images=None  # Set to None to process all images, or a number to limit\n",
    "    )\n",
    "    print(f\"Processed {count} CelebDF images\")\n",
    "else:\n",
    "    print(\"Skipping CelebDF processing. Set PROCESS_DATASETS=True to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 5: FACEFORENSICS++ DATASET PROCESSING\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "FaceForensics++ Dataset Processing\n",
    "-------------------------------\n",
    "Process the FaceForensics++ dataset by extracting face folders,\n",
    "resizing images, and saving them to a standardized format.\n",
    "\"\"\"\n",
    "def process_faceforensics_dataset(source_dir, target_dir, target_size=(128, 128), max_folders=None, max_images_per_folder=None):\n",
    "    \"\"\"Process FaceForensics++ dataset where images are organized in numbered folders\"\"\"\n",
    "    source_path = Path(source_dir)\n",
    "    target_path = Path(target_dir)\n",
    "    \n",
    "    if not source_path.exists():\n",
    "        print(f\"Source directory {source_path} does not exist.\")\n",
    "        return 0\n",
    "    \n",
    "    # Get all folders in the source directory\n",
    "    face_folders = [f for f in source_path.iterdir() if f.is_dir()]\n",
    "    \n",
    "    # Limit number of folders if specified\n",
    "    if max_folders and len(face_folders) > max_folders:\n",
    "        face_folders = random.sample(face_folders, max_folders)\n",
    "    \n",
    "    print(f\"Found {len(face_folders)} face folders in {source_path}\")\n",
    "    \n",
    "    # Count total images to process for progress bar\n",
    "    total_images = 0\n",
    "    folder_image_counts = []\n",
    "    for folder in face_folders:\n",
    "        image_files = []\n",
    "        for ext in ['.jpg', '.jpeg', '.png']:\n",
    "            image_files.extend(folder.glob(f\"*{ext}\"))\n",
    "            image_files.extend(folder.glob(f\"*{ext.upper()}\"))\n",
    "        \n",
    "        count = len(image_files)\n",
    "        if max_images_per_folder and count > max_images_per_folder:\n",
    "            count = max_images_per_folder\n",
    "        \n",
    "        folder_image_counts.append(count)\n",
    "        total_images += count\n",
    "    \n",
    "    # Process images in each folder with progress bar\n",
    "    count = 0\n",
    "    with tqdm(total=total_images, desc=\"Processing FaceForensics++ images\", unit=\"img\") as pbar:\n",
    "        for folder_idx, folder in enumerate(face_folders):\n",
    "            # Get image files in this folder\n",
    "            image_files = []\n",
    "            for ext in ['.jpg', '.jpeg', '.png']:\n",
    "                image_files.extend(folder.glob(f\"*{ext}\"))\n",
    "                image_files.extend(folder.glob(f\"*{ext.upper()}\"))\n",
    "            \n",
    "            # Limit number of images per folder if specified\n",
    "            if max_images_per_folder and len(image_files) > max_images_per_folder:\n",
    "                image_files = random.sample(image_files, max_images_per_folder)\n",
    "            \n",
    "                try:\n",
    "                    # Open and resize image\n",
    "                    img = Image.open(img_path)\n",
    "                    img = img.resize(target_size, Image.LANCZOS)\n",
    "                    \n",
    "                    # Save to target directory with folder index as face ID\n",
    "                    target_file = target_path / f\"ff_{folder_idx:04d}_{img_idx:04d}{img_path.suffix}\"\n",
    "                    img.save(target_file)\n",
    "                    count += 1\n",
    "                    pbar.update(1)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing {img_path.name}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully processed {count} images from FaceForensics++ to {target_path}\")\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 999 face folders in c:\\Users\\vinay\\Documents\\mnist\\faces\\Real\\FaceForensics++\\original_sequences\\youtube\\c23\\frames\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6030fc84d77b49b5a12d04739f06b405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing FaceForensics++ images:   0%|          | 0/63898 [00:00<?, ?img/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 63898 images from FaceForensics++ to c:\\Users\\vinay\\Documents\\mnist\\processed_faces\\faceforensics\n",
      "Processed 63898 FaceForensics++ images\n"
     ]
    }
   ],
   "source": [
    "# Only run if PROCESS_DATASETS is True and FaceForensics++ dataset exists\n",
    "if PROCESS_DATASETS and ff_exists:\n",
    "    count = process_faceforensics_dataset(\n",
    "        FF_PATH, \n",
    "        ff_dir, \n",
    "        target_size=(TARGET_SIZE, TARGET_SIZE),\n",
    "        max_folders=FF_MAX_FACES,\n",
    "        max_images_per_folder=FF_MAX_PER_FACE\n",
    "    )\n",
    "    print(f\"Processed {count} FaceForensics++ images\")\n",
    "else:\n",
    "    print(\"Skipping FaceForensics++ processing. Set PROCESS_DATASETS=True to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining 80576 CelebDF images and 63898 FaceForensics++ images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01457ff49bfb41ab9c2092904fc3e16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combining datasets:   0%|          | 0/144474 [00:00<?, ?img/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset created with 144474 total images at c:\\Users\\vinay\\Documents\\mnist\\processed_faces\\combined\n",
      "Combined dataset created with 144474 images\n",
      "Dataset path for training: c:\\Users\\vinay\\Documents\\mnist\\processed_faces\\combined\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# CELL 6: COMBINE DATASETS\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Dataset Combination\n",
    "----------------\n",
    "Combine the processed CelebDF and FaceForensics++ datasets \n",
    "into a single dataset for training. This ensures we have a diverse\n",
    "set of facial images.\n",
    "\"\"\"\n",
    "def combine_datasets(celebdf_dir, ff_dir, combined_dir):\n",
    "    \"\"\"Combine processed datasets into one directory\"\"\"\n",
    "    # Copy all images from CelebDF directory\n",
    "    celebdf_files = list(Path(celebdf_dir).glob(\"*.jpg\")) + list(Path(celebdf_dir).glob(\"*.png\"))\n",
    "    ff_files = list(Path(ff_dir).glob(\"*.jpg\")) + list(Path(ff_dir).glob(\"*.png\"))\n",
    "    \n",
    "    total_files = len(celebdf_files) + len(ff_files)\n",
    "    print(f\"Combining {len(celebdf_files)} CelebDF images and {len(ff_files)} FaceForensics++ images...\")\n",
    "    \n",
    "    with tqdm(total=total_files, desc=\"Combining datasets\", unit=\"img\") as pbar:\n",
    "        # Copy CelebDF images\n",
    "        for img_path in celebdf_files:\n",
    "            try:\n",
    "                target_path = Path(combined_dir) / img_path.name\n",
    "                img = Image.open(img_path)\n",
    "                img.save(target_path)\n",
    "                pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError copying {img_path.name}: {e}\")\n",
    "        \n",
    "        # Copy FaceForensics++ images\n",
    "        for img_path in ff_files:\n",
    "            try:\n",
    "                target_path = Path(combined_dir) / img_path.name\n",
    "                img = Image.open(img_path)\n",
    "                img.save(target_path)\n",
    "                pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError copying {img_path.name}: {e}\")\n",
    "    \n",
    "    print(f\"Combined dataset created with {total_files} total images at {combined_dir}\")\n",
    "    return total_files\n",
    "\n",
    "# Only run if both datasets have been processed\n",
    "if PROCESS_DATASETS and celebdf_exists and ff_exists:\n",
    "    total_images = combine_datasets(celebdf_dir, ff_dir, combined_dir)\n",
    "    dataset_path = combined_dir\n",
    "    print(f\"Combined dataset created with {total_images} images\")\n",
    "elif celebdf_exists and os.path.exists(celebdf_dir) and len(os.listdir(celebdf_dir)) > 0:\n",
    "    dataset_path = celebdf_dir\n",
    "    print(f\"Using CelebDF dataset at {celebdf_dir}\")\n",
    "elif ff_exists and os.path.exists(ff_dir) and len(os.listdir(ff_dir)) > 0:\n",
    "    dataset_path = ff_dir\n",
    "    print(f\"Using FaceForensics++ dataset at {ff_dir}\")\n",
    "elif os.path.exists(combined_dir) and len(os.listdir(combined_dir)) > 0:\n",
    "    dataset_path = combined_dir\n",
    "    print(f\"Using existing combined dataset at {combined_dir}\")\n",
    "else:\n",
    "    print(\"Error: No datasets were processed or found\")\n",
    "    dataset_path = None\n",
    "\n",
    "print(f\"Dataset path for training: {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu126\n",
      "CUDA version: 12.6\n",
      "GPU: NVIDIA GeForce RTX 2070 Super with Max-Q Design\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# CELL 7: DATASET CLASS AND CUDA SETUP\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Dataset Class and CUDA Setup\n",
    "-------------------------\n",
    "Define the custom dataset class for loading face images\n",
    "and set up CUDA for GPU acceleration if available.\n",
    "\"\"\"\n",
    "class FaceDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading face images\"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(root_dir) \n",
    "                           if os.path.isfile(os.path.join(root_dir, f)) and \n",
    "                           f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, 0  # Return 0 as dummy label\n",
    "\n",
    "def setup_cuda():\n",
    "    \"\"\"Setup CUDA if available\"\"\"\n",
    "    global CUDA, device\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        CUDA = False\n",
    "        print(\"CUDA is not available. Running on CPU.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "    else:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        # Enable deterministic behavior for reproducibility\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        cudnn.benchmark = True\n",
    "    \n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    if CUDA:\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device\n",
    "\n",
    "# Setup CUDA\n",
    "device = setup_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator initialized\n",
      "Discriminator initialized\n",
      "\n",
      "Generator Architecture:\n",
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (16): Tanh()\n",
      "  )\n",
      ")\n",
      "\n",
      "Discriminator Architecture:\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (12): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (14): Conv2d(1024, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (15): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# CELL 8: GAN MODEL ARCHITECTURE\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "GAN Model Architecture\n",
    "-------------------\n",
    "Define the Generator and Discriminator architecture\n",
    "for the Deep Convolutional GAN (DCGAN).\n",
    "\"\"\"\n",
    "def weights_init(m):\n",
    "    \"\"\"Initialize network weights\"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator Network\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: Z_DIM x 1 x 1\n",
    "            nn.ConvTranspose2d(Z_DIM, G_HIDDEN * 16, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN * 16),\n",
    "            nn.ReLU(True),\n",
    "            # 4x4\n",
    "            \n",
    "            nn.ConvTranspose2d(G_HIDDEN * 16, G_HIDDEN * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN * 8),\n",
    "            nn.ReLU(True),\n",
    "            # 8x8\n",
    "            \n",
    "            nn.ConvTranspose2d(G_HIDDEN * 8, G_HIDDEN * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN * 4),\n",
    "            nn.ReLU(True),\n",
    "            # 16x16\n",
    "            \n",
    "            nn.ConvTranspose2d(G_HIDDEN * 4, G_HIDDEN * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN * 2),\n",
    "            nn.ReLU(True),\n",
    "            # 32x32\n",
    "            \n",
    "            nn.ConvTranspose2d(G_HIDDEN * 2, G_HIDDEN, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN),\n",
    "            nn.ReLU(True),\n",
    "            # 64x64\n",
    "            \n",
    "            nn.ConvTranspose2d(G_HIDDEN, IMAGE_CHANNEL, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # 128x128\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator Network\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: IMAGE_CHANNEL x 128 x 128\n",
    "            nn.Conv2d(IMAGE_CHANNEL, D_HIDDEN, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 64x64\n",
    "            \n",
    "            nn.Conv2d(D_HIDDEN, D_HIDDEN * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 32x32\n",
    "            \n",
    "            nn.Conv2d(D_HIDDEN * 2, D_HIDDEN * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 16x16\n",
    "            \n",
    "            nn.Conv2d(D_HIDDEN * 4, D_HIDDEN * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 8x8\n",
    "            \n",
    "            nn.Conv2d(D_HIDDEN * 8, D_HIDDEN * 16, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 4x4\n",
    "            \n",
    "            nn.Conv2d(D_HIDDEN * 16, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # 1x1\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input).view(-1, 1).squeeze(1)\n",
    "\n",
    "# Initialize networks\n",
    "netG = Generator().to(device)\n",
    "netG.apply(weights_init)\n",
    "print(\"Generator initialized\")\n",
    "\n",
    "netD = Discriminator().to(device)\n",
    "netD.apply(weights_init)\n",
    "print(\"Discriminator initialized\")\n",
    "\n",
    "# Display network architecture summaries\n",
    "print(\"\\nGenerator Architecture:\")\n",
    "print(netG)\n",
    "print(\"\\nDiscriminator Architecture:\")\n",
    "print(netD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 144473 images\n",
      "Number of batches: 4515\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# CELL 9: DATA LOADING AND VISUALIZATION\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Data Loading and Visualization\n",
    "---------------------------\n",
    "Load the processed face dataset, apply transformations,\n",
    "and visualize sample images.\n",
    "\"\"\"\n",
    "def load_dataset(data_path):\n",
    "    \"\"\"Load face dataset and create dataloader\"\"\"\n",
    "    # Image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(X_DIM),\n",
    "        transforms.CenterCrop(X_DIM),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize for RGB\n",
    "    ])\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    try:\n",
    "        dataset = FaceDataset(root_dir=data_path, transform=transform)\n",
    "        if len(dataset) == 0:\n",
    "            print(f\"Error: No images found in {data_path}\")\n",
    "            return None, None\n",
    "            \n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=True,\n",
    "            num_workers=2, \n",
    "            pin_memory=True if CUDA else False\n",
    "        )\n",
    "        print(f\"Dataset loaded with {len(dataset)} images\")\n",
    "        print(f\"Number of batches: {len(dataloader)}\")\n",
    "        return dataset, dataloader\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def visualize_dataset(dataloader, output_path, title=\"Training Images\"):\n",
    "    \"\"\"Save sample images from dataset\"\"\"\n",
    "    try:\n",
    "        real_batch = next(iter(dataloader))[0]\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(title)\n",
    "        plt.imshow(np.transpose(vutils.make_grid(\n",
    "            real_batch[:min(16, len(real_batch))], padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "        filepath = os.path.join(output_path, \"real_samples.png\")\n",
    "        plt.savefig(filepath)\n",
    "        plt.show()\n",
    "        print(f\"Saved sample of real images to {filepath}\")\n",
    "        return filepath\n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load dataset if path exists\n",
    "if dataset_path and os.path.exists(dataset_path):\n",
    "    dataset, dataloader = load_dataset(dataset_path)\n",
    "    if dataset and dataloader:\n",
    "        # Visualize some samples\n",
    "        visualize_dataset(dataloader, OUTPUT_PATH, \"Sample Training Images\")\n",
    "else:\n",
    "    print(\"Dataset path not available. Please check your configurations.\")\n",
    "    dataloader = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 10: CHECKPOINT FUNCTIONS\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Checkpoint Functions\n",
    "----------------\n",
    "Functions for saving and loading model checkpoints\n",
    "to support resumable training and best model selection.\n",
    "\"\"\"\n",
    "def save_checkpoint(netG, netD, optimG, optimD, epoch, iteration, losses, img_list, filename=None):\n",
    "    \"\"\"Save a checkpoint of the current training state\"\"\"\n",
    "    if not filename:\n",
    "        filename = os.path.join(CHECKPOINT_DIR, f\"checkpoint_e{epoch}_i{iteration}.pt\")\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'iteration': iteration,\n",
    "        'netG_state_dict': netG.state_dict(),\n",
    "        'netD_state_dict': netD.state_dict(),\n",
    "        'optimG_state_dict': optimG.state_dict(),\n",
    "        'optimD_state_dict': optimD.state_dict(),\n",
    "        'G_losses': losses['G'],\n",
    "        'D_losses': losses['D'],\n",
    "        'img_list': img_list,\n",
    "        'timestamp': datetime.datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved: {filename}\")\n",
    "    \n",
    "    # Save the latest checkpoint also (for resuming)\n",
    "    latest_path = os.path.join(CHECKPOINT_DIR, \"latest_checkpoint.pt\")\n",
    "    torch.save(checkpoint, latest_path)\n",
    "    \n",
    "    return filename\n",
    "\n",
    "def load_checkpoint(netG, netD, optimG, optimD, filename=None):\n",
    "    \"\"\"Load a checkpoint to resume training\"\"\"\n",
    "    if not filename:\n",
    "        # Try to load the latest checkpoint\n",
    "        filename = os.path.join(CHECKPOINT_DIR, \"latest_checkpoint.pt\")\n",
    "    \n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"No checkpoint found at {filename}\")\n",
    "        return None, 0, 0, {'G': [], 'D': []}, []\n",
    "    \n",
    "    print(f\"Loading checkpoint: {filename}\")\n",
    "    checkpoint = torch.load(filename, map_location=device)\n",
    "    \n",
    "    netG.load_state_dict(checkpoint['netG_state_dict'])\n",
    "    netD.load_state_dict(checkpoint['netD_state_dict'])\n",
    "    optimG.load_state_dict(checkpoint['optimG_state_dict'])\n",
    "    optimD.load_state_dict(checkpoint['optimD_state_dict'])\n",
    "    \n",
    "    # For optimizers loaded from checkpoint, move to correct device\n",
    "    if CUDA:\n",
    "        for state in optimG.state.values():\n",
    "            for k, v in state.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    state[k] = v.to(device)\n",
    "                    \n",
    "        for state in optimD.state.values():\n",
    "            for k, v in state.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    state[k] = v.to(device)\n",
    "    \n",
    "    epoch = checkpoint['epoch']\n",
    "    iteration = checkpoint['iteration']\n",
    "    G_losses = checkpoint['G_losses']\n",
    "    D_losses = checkpoint['D_losses']\n",
    "    img_list = checkpoint.get('img_list', [])\n",
    "    \n",
    "    print(f\"Resuming from epoch {epoch+1}, iteration {iteration}\")\n",
    "    \n",
    "    return {'netG': netG, 'netD': netD, 'optimG': optimG, 'optimD': optimD}, epoch, iteration, {'G': G_losses, 'D': D_losses}, img_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 11: TRAINING INITIALIZATION\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Training Initialization\n",
    "--------------------\n",
    "Initialize GAN training by setting up optimizers, \n",
    "creating fixed noise for visualization,\n",
    "and loading a checkpoint if resuming training.\n",
    "\"\"\"\n",
    "# Setup loss function and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "# Fixed noise for visualization\n",
    "fixed_noise = torch.randn(16, Z_DIM, 1, 1, device=device)\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Setup TensorBoard\n",
    "if ENABLE_TENSORBOARD:\n",
    "    tb_writer = SummaryWriter(log_dir=os.path.join(LOG_DIR, EXPERIMENT_NAME))\n",
    "    # Add model graph to TensorBoard\n",
    "    try:\n",
    "        sample_input = torch.randn(1, Z_DIM, 1, 1, device=device)\n",
    "        tb_writer.add_graph(netG, sample_input)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not add model graph to TensorBoard: {e}\")\n",
    "else:\n",
    "    tb_writer = None\n",
    "\n",
    "# Try to load checkpoint if resuming training\n",
    "start_epoch = 0\n",
    "start_iter = 0\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "img_list = []\n",
    "\n",
    "if RESUME_TRAINING:\n",
    "    models, start_epoch, start_iter, losses, prev_img_list = load_checkpoint(netG, netD, optimizerG, optimizerD)\n",
    "    if models:  # If checkpoint was loaded successfully\n",
    "        netG, netD = models['netG'], models['netD']\n",
    "        optimizerG, optimizerD = models['optimG'], models['optimD']\n",
    "        G_losses = losses['G']\n",
    "        D_losses = losses['D']\n",
    "        img_list = prev_img_list\n",
    "        print(f\"Resuming from epoch {start_epoch+1}, iteration {start_iter}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\\nREADY FOR TRAINING\\n{'='*50}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 12: TRAINING LOOP\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "GAN Training Loop\n",
    "--------------\n",
    "Train the GAN model with comprehensive tracking, visualization,\n",
    "early stopping, and checkpoint saving. This cell runs the main \n",
    "training process through all epochs.\n",
    "\"\"\"\n",
    "def train_epoch(epoch, iters):\n",
    "    \"\"\"Train the GAN for one epoch\"\"\"\n",
    "    # Variables for this epoch\n",
    "    epoch_g_loss = 0.0\n",
    "    epoch_d_loss = 0.0\n",
    "    batch_count = 0\n",
    "    \n",
    "    # Time tracking\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tqdm(dataloader, unit=\"batch\", desc=f\"Epoch {epoch+1}/{EPOCH_NUM}\", dynamic_ncols=True) as tepoch:\n",
    "        for i, data in enumerate(tepoch):\n",
    "            real_images = data[0].to(device, non_blocking=True)\n",
    "            b_size = real_images.size(0)\n",
    "            \n",
    "            # Train Discriminator with real images\n",
    "            netD.zero_grad()\n",
    "            label = torch.full((b_size,), REAL_LABEL, dtype=torch.float, device=device)\n",
    "            output = netD(real_images)\n",
    "            errD_real = criterion(output, label)\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "            \n",
    "            # Train Discriminator with fake images\n",
    "            noise = torch.randn(b_size, Z_DIM, 1, 1, device=device)\n",
    "            fake = netG(noise)\n",
    "            label.fill_(FAKE_LABEL)\n",
    "            output = netD(fake.detach())\n",
    "            errD_fake = criterion(output, label)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            \n",
    "            errD = errD_real + errD_fake\n",
    "            optimizerD.step()\n",
    "            \n",
    "            # Train Generator\n",
    "            netG.zero_grad()\n",
    "            label.fill_(REAL_LABEL)  # Fake labels are real for generator cost\n",
    "            output = netD(fake)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimizerG.step()\n",
    "            \n",
    "            # Save losses for plotting\n",
    "            G_losses.append(errG.item())\n",
    "            D_losses.append(errD.item())\n",
    "            \n",
    "            # Accumulate losses for this epoch\n",
    "            epoch_g_loss += errG.item()\n",
    "            epoch_d_loss += errD.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Add losses to TensorBoard\n",
    "            if ENABLE_TENSORBOARD:\n",
    "                tb_writer.add_scalar('Loss/Generator', errG.item(), iters)\n",
    "                tb_writer.add_scalar('Loss/Discriminator', errD.item(), iters)\n",
    "                tb_writer.add_scalar('D_x', D_x, iters)\n",
    "                tb_writer.add_scalar('D_G_z1', D_G_z1, iters)\n",
    "                tb_writer.add_scalar('D_G_z2', D_G_z2, iters)\n",
    "            \n",
    "            # Resource monitoring\n",
    "            if RESOURCE_MONITORING and iters % RESOURCE_CHECK_FREQ == 0:\n",
    "                resources = monitor_resources()\n",
    "                print(f\"\\nResource usage: CPU {resources['cpu_percent']}%, Memory {resources['memory_percent']}%\")\n",
    "                print(f\"GPU: {resources['gpu_info']}\")\n",
    "                \n",
    "                if ENABLE_TENSORBOARD:\n",
    "                    tb_writer.add_scalar('Resources/CPU', resources['cpu_percent'], iters)\n",
    "                    tb_writer.add_scalar('Resources/Memory', resources['memory_percent'], iters)\n",
    "            \n",
    "            # Save generated images periodically\n",
    "            if (iters % CHECKPOINT_SAMPLES == 0) or ((epoch == EPOCH_NUM-1) and (i == len(dataloader)-1)):\n",
    "                with torch.no_grad():\n",
    "                    fake = netG(fixed_noise).detach().cpu()\n",
    "                img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "                \n",
    "                # Save current generator output\n",
    "                plt.figure(figsize=(8,8))\n",
    "                plt.axis(\"off\")\n",
    "                plt.title(f\"Generated Images (Epoch {epoch+1}, Iter {iters})\")\n",
    "                plt.imshow(np.transpose(img_list[-1], (1,2,0)))\n",
    "                plt.savefig(os.path.join(OUTPUT_PATH, f\"generated_e{epoch+1}_i{iters}.png\"))\n",
    "                plt.close()\n",
    "                \n",
    "                # Add images to TensorBoard\n",
    "                if ENABLE_TENSORBOARD:\n",
    "                    tb_writer.add_image('Generated Images', np.transpose(img_list[-1], (2, 0, 1)), iters)\n",
    "            \n",
    "            iters += 1\n",
    "    \n",
    "    # Calculate average losses for this epoch\n",
    "    if batch_count > 0:\n",
    "        epoch_g_loss /= batch_count\n",
    "        epoch_d_loss /= batch_count\n",
    "        \n",
    "    return epoch_g_loss, epoch_d_loss, iters\n",
    "\n",
    "# Variables for early stopping\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_path = None\n",
    "\n",
    "# Start training from the last epoch if resuming\n",
    "iters = start_iter\n",
    "for epoch in range(start_epoch, EPOCH_NUM):\n",
    "    # Train for one epoch\n",
    "    epoch_g_loss, epoch_d_loss, iters = train_epoch(epoch, iters)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} average losses - Generator: {epoch_g_loss:.4f}, Discriminator: {epoch_d_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoint at the end of each epoch\n",
    "    if (epoch + 1) % CHECKPOINT_FREQ == 0:\n",
    "        checkpoint_path = save_checkpoint(\n",
    "            netG, netD, optimizerG, optimizerD, epoch, \n",
    "            iters, {'G': G_losses, 'D': D_losses}, img_list\n",
    "        )\n",
    "    \n",
    "    # Early stopping check based on generator loss\n",
    "    current_loss = epoch_g_loss\n",
    "    if current_loss < best_loss - EARLY_STOPPING_THRESHOLD:\n",
    "        best_loss = current_loss\n",
    "        patience_counter = 0\n",
    "        # Save the best model\n",
    "        best_model_path = save_checkpoint(\n",
    "            netG, netD, optimizerG, optimizerD, epoch, \n",
    "            iters, {'G': G_losses, 'D': D_losses}, img_list,\n",
    "            os.path.join(CHECKPOINT_DIR, \"best_model.pt\")\n",
    "        )\n",
    "        print(f\"New best model saved with G loss: {best_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Early stopping patience: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "        \n",
    "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"Early stopping triggered! No improvement for {EARLY_STOPPING_PATIENCE} epochs.\")\n",
    "            break\n",
    "\n",
    "# Final checkpoint\n",
    "save_checkpoint(\n",
    "    netG, netD, optimizerG, optimizerD, \n",
    "    min(epoch, EPOCH_NUM-1), iters, \n",
    "    {'G': G_losses, 'D': D_losses}, img_list, \n",
    "    os.path.join(CHECKPOINT_DIR, \"final_model.pt\")\n",
    ")\n",
    "\n",
    "# Close TensorBoard writer\n",
    "if ENABLE_TENSORBOARD and tb_writer:\n",
    "    tb_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 13: VISUALIZATIONS\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Result Visualizations\n",
    "------------------\n",
    "Generate visualizations of training progress, loss curves,\n",
    "and sample images from the trained generator.\n",
    "\"\"\"\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss\")\n",
    "plt.plot(G_losses, label=\"Generator\")\n",
    "plt.plot(D_losses, label=\"Discriminator\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, \"loss_curves.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Visualize training progress images\n",
    "def visualize_training_progress(img_list, output_path, title=\"Training Progress\"):\n",
    "    \"\"\"Create a grid of generated images showing progression\"\"\"\n",
    "    if not img_list:\n",
    "        print(\"No images available for progress visualization\")\n",
    "        return None\n",
    "        \n",
    "    # Select evenly spaced samples to show progression\n",
    "    num_samples = min(9, len(img_list))\n",
    "    step = max(1, len(img_list) // num_samples)\n",
    "    samples = [img_list[i] for i in range(0, len(img_list), step)][:num_samples]\n",
    "    \n",
    "    # Create a grid\n",
    "    rows = int(np.sqrt(len(samples)))\n",
    "    cols = int(np.ceil(len(samples) / rows))\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, img in enumerate(samples):\n",
    "        if i < len(axes):\n",
    "            sample_num = i * step\n",
    "            if sample_num >= len(img_list):\n",
    "                sample_num = len(img_list) - 1\n",
    "            axes[i].imshow(np.transpose(img, (1, 2, 0)))\n",
    "            axes[i].set_title(f\"Sample {sample_num}\")\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(samples), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filepath = os.path.join(output_path, \"training_progress.png\")\n",
    "    plt.savefig(filepath)\n",
    "    plt.show()\n",
    "    print(f\"Saved training progress visualization to {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "# Show progress if we have multiple images\n",
    "if len(img_list) > 1:\n",
    "    visualize_training_progress(img_list, OUTPUT_PATH)\n",
    "\n",
    "# Final visualization: Real vs Generated\n",
    "real_batch = next(iter(dataloader))[0][:16].to(device)\n",
    "\n",
    "# Generate a batch of fake images\n",
    "with torch.no_grad():\n",
    "    fake_batch = netG(fixed_noise).detach().cpu()\n",
    "\n",
    "# Plot real images vs fake images\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch.cpu(), padding=2, normalize=True),(1,2,0)))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(fake_batch, padding=2, normalize=True),(1,2,0)))\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, \"final_comparison.png\"))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "# CELL 14: GENERATE SAMPLES WITH TRAINED MODEL\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Generate Samples with Trained Model\n",
    "---------------------------------\n",
    "Generate and visualize multiple samples using the trained generator.\n",
    "This demonstrates how to use the model for inference.\n",
    "\"\"\"\n",
    "def generate_samples(netG, n_samples=16, grid_rows=4):\n",
    "    \"\"\"Generate multiple samples from the trained generator\"\"\"\n",
    "    grid_cols = n_samples // grid_rows\n",
    "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(grid_cols*2, grid_rows*2))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(n_samples):\n",
    "            # Generate a random latent vector\n",
    "            z = torch.randn(1, Z_DIM, 1, 1, device=device)\n",
    "            \n",
    "            # Generate a fake image\n",
    "            fake = netG(z).detach().cpu()\n",
    "            \n",
    "            # Display the image\n",
    "            img = np.transpose(vutils.make_grid(fake, padding=0, normalize=True), (1, 2, 0))\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_PATH, \"generated_samples.png\"))\n",
    "    plt.show()\n",
    "    print(f\"Generated {n_samples} samples using the trained model\")\n",
    "\n",
    "# Generate samples with the trained model\n",
    "generate_samples(netG, n_samples=16, grid_rows=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 15: CREATE LATENT SPACE ANIMATION\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Latent Space Animation\n",
    "-------------------\n",
    "Create an animation by interpolating between points in the latent space.\n",
    "This helps visualize how the latent space is structured.\n",
    "\"\"\"\n",
    "def latent_space_interpolation(netG, n_steps=30):\n",
    "    \"\"\"Create an animation by interpolating between two latent vectors\"\"\"\n",
    "    # Generate two random latent vectors\n",
    "    z_start = torch.randn(1, Z_DIM, 1, 1, device=device)\n",
    "    z_end = torch.randn(1, Z_DIM, 1, 1, device=device)\n",
    "    \n",
    "    # Generate intermediate vectors by linear interpolation\n",
    "    z_vectors = [z_start + (z_end - z_start) * (step / (n_steps-1)) for step in range(n_steps)]\n",
    "    \n",
    "    # Generate images for each interpolated vector\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.axis(\"off\")\n",
    "    images = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for z in tqdm(z_vectors, desc=\"Generating interpolated images\"):\n",
    "            fake = netG(z).detach().cpu()\n",
    "            img = vutils.make_grid(fake, padding=2, normalize=True)\n",
    "            images.append([plt.imshow(np.transpose(img, (1, 2, 0)), animated=True)])\n",
    "    \n",
    "    # Create animation\n",
    "    ani = animation.ArtistAnimation(fig, images, interval=200, blit=True)\n",
    "    ani.save(os.path.join(OUTPUT_PATH, \"latent_space_animation.gif\"), writer='pillow', fps=10)\n",
    "    \n",
    "    # Display the first and last images side by side\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Start Point\")\n",
    "    with torch.no_grad():\n",
    "        fake = netG(z_start).detach().cpu()\n",
    "    plt.imshow(np.transpose(vutils.make_grid(fake, padding=2, normalize=True), (1, 2, 0)))\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"End Point\")\n",
    "    with torch.no_grad():\n",
    "        fake = netG(z_end).detach().cpu()\n",
    "    plt.imshow(np.transpose(vutils.make_grid(fake, padding=2, normalize=True), (1, 2, 0)))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_PATH, \"latent_space_endpoints.png\"))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Latent space animation saved to {os.path.join(OUTPUT_PATH, 'latent_space_animation.gif')}\")\n",
    "    return ani\n",
    "\n",
    "# Create a latent space interpolation animation (fewer steps for quicker execution)\n",
    "latent_space_interpolation(netG, n_steps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 16: SAVE FINAL MODELS\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Save Final Models\n",
    "--------------\n",
    "Save the trained generator and discriminator models for later use.\n",
    "This allows you to load and use the models without retraining.\n",
    "\"\"\"\n",
    "# Save the final models\n",
    "torch.save(netG.state_dict(), os.path.join(OUTPUT_PATH, \"generator_final.pth\"))\n",
    "torch.save(netD.state_dict(), os.path.join(OUTPUT_PATH, \"discriminator_final.pth\"))\n",
    "\n",
    "print(f\"Final models saved to {OUTPUT_PATH}\")\n",
    "print(\"- generator_final.pth\")\n",
    "print(\"- discriminator_final.pth\")\n",
    "\n",
    "# Function to load the trained generator for inference\n",
    "def load_generator(model_path):\n",
    "    \"\"\"Load a trained generator model\"\"\"\n",
    "    model = Generator().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# Example of how to load and use the saved generator\n",
    "\"\"\"\n",
    "print(\"\\nExample of loading the saved generator:\")\n",
    "print(\"loaded_generator = load_generator(os.path.join(OUTPUT_PATH, 'generator_final.pth'))\")\n",
    "print(\"# Generate images with loaded model:\")\n",
    "print(\"with torch.no_grad():\")\n",
    "print(\"    z = torch.randn(1, Z_DIM, 1, 1, device=device)\")\n",
    "print(\"    fake_img = loaded_generator(z)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 17: TRAINING SUMMARY AND METRICS\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Training Summary and Metrics\n",
    "-------------------------\n",
    "Display a summary of the training process including timing information,\n",
    "final loss values, and resource usage statistics.\n",
    "\"\"\"\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Calculate total images processed\n",
    "total_images = len(dataset) * EPOCH_NUM if dataset else 0\n",
    "\n",
    "# Get final losses (average of last 100 iterations)\n",
    "final_g_loss = np.mean(G_losses[-100:]) if G_losses else float('nan')\n",
    "print(f\"{'='*50}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
