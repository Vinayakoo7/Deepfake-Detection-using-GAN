{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 1: IMPORTS AND LIBRARIES\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Face GAN - Deep Convolutional GAN for Face Generation\n",
    "----------------------------------------------------\n",
    "Import required libraries and modules for dataset processing,\n",
    "neural network creation, training, and visualization.\n",
    "\"\"\"\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter progress bars\n",
    "\n",
    "# Try importing optional libraries\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    TENSORBOARD_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TENSORBOARD_AVAILABLE = False\n",
    "    print(\"TensorBoard not available. Install with: pip install tensorboard\")\n",
    "\n",
    "# For resource monitoring\n",
    "try:\n",
    "    import psutil\n",
    "    import GPUtil\n",
    "    RESOURCE_MONITORING = True\n",
    "except ImportError:\n",
    "    RESOURCE_MONITORING = False\n",
    "    print(\"Resource monitoring unavailable. Install with: pip install psutil gputil\")\n",
    "\n",
    "# Set display settings for the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset paths:\n",
      "- CelebDF: c:\\Users\\vinay\\Documents\\mnist\\faces\\Real\\Celeb_V2\\Train\\real\n",
      "- FaceForensics++: c:\\Users\\vinay\\Documents\\mnist\\faces\\Real\\FaceForensics++\\original_sequences\\youtube\\c23\\frames\n",
      "Processed data will be saved to: c:\\Users\\vinay\\Documents\\mnist\\processed_faces\n",
      "Output will be saved to: c:\\Users\\vinay\\Documents\\mnist\\output\n",
      "Target image size: 128x128\n",
      "Batch size: 64\n",
      "Training epochs: 10\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# CELL 2: CONFIGURATION SETTINGS\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Configuration Settings\n",
    "---------------------\n",
    "Define paths, hyperparameters, and training settings.\n",
    "Modify these values to adapt the model to your specific requirements.\n",
    "\"\"\"\n",
    "# Paths\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "CELEBDF_PATH = os.path.join(BASE_DIR, \"faces\\\\Real\\\\Celeb_V2\\\\Train\\\\real\")\n",
    "FF_PATH = os.path.join(BASE_DIR, \"faces\\\\Real\\\\FaceForensics++\\\\original_sequences\\\\youtube\\\\c23\\\\frames\")\n",
    "PROCESSED_PATH = os.path.join(BASE_DIR, \"processed_faces\")\n",
    "OUTPUT_PATH = os.path.join(BASE_DIR, \"output\")\n",
    "CHECKPOINT_DIR = os.path.join(OUTPUT_PATH, \"checkpoints\")\n",
    "LOG_DIR = os.path.join(OUTPUT_PATH, \"logs\")\n",
    "\n",
    "# Dataset processing settings\n",
    "PROCESS_DATASETS = False  # Set to False to skip dataset processing if already processed\n",
    "CELEBDF_MAX_PER_FACE = None  # Max images per face for CelebDF\n",
    "FF_MAX_PER_FACE = None  # Max images per face for FaceForensics++\n",
    "FF_MAX_FACES = None  # Maximum number of different faces from FaceForensics++\n",
    "TARGET_SIZE = 128  # Size to resize images to\n",
    "\n",
    "# Model hyperparameters\n",
    "CUDA = True  # Use CUDA (will be auto-detected later)\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_CHANNEL = 3  # RGB images\n",
    "Z_DIM = 100  # Latent vector dimension\n",
    "G_HIDDEN = 64  # Double from 64 to 128\n",
    "D_HIDDEN = 64  # Double from 64 to 128\n",
    "X_DIM = 128  # Target image size\n",
    "EPOCH_NUM = 10  # Number of training epochs\n",
    "REAL_LABEL = 1\n",
    "FAKE_LABEL = 0\n",
    "lr = 2e-4  # Learning rate\n",
    "seed = 1  # Random seed for reproducibility\n",
    "\n",
    "# Training control settings\n",
    "CHECKPOINT_FREQ = 5  # Save checkpoints every N epochs\n",
    "CHECKPOINT_SAMPLES = 1000  # Generate image samples every N iterations\n",
    "RESUME_TRAINING = True  # Try to resume from checkpoint if available\n",
    "EARLY_STOPPING_PATIENCE = 5  # Early stopping after N epochs without improvement\n",
    "EARLY_STOPPING_THRESHOLD = 0.01  # Minimum improvement to reset patience counter\n",
    "RESOURCE_CHECK_FREQ = 50  # Check system resources every N batches\n",
    "MAX_TRAINING_TIME = None  # Max training time in hours (None for no limit)\n",
    "ENABLE_TENSORBOARD = TENSORBOARD_AVAILABLE  # Enable TensorBoard logging\n",
    "EXPERIMENT_NAME = f\"face_gan_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"  # Unique name for this run\n",
    "\n",
    "# Print some key configuration values\n",
    "print(f\"Dataset paths:\\n- CelebDF: {CELEBDF_PATH}\\n- FaceForensics++: {FF_PATH}\")\n",
    "print(f\"Processed data will be saved to: {PROCESSED_PATH}\")\n",
    "print(f\"Output will be saved to: {OUTPUT_PATH}\")\n",
    "print(f\"Target image size: {TARGET_SIZE}x{TARGET_SIZE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training epochs: {EPOCH_NUM}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output directories:\n",
      " - CelebDF: c:\\Users\\vinay\\Documents\\mnist\\processed_faces\\celebdf\n",
      " - FaceForensics++: c:\\Users\\vinay\\Documents\\mnist\\processed_faces\\faceforensics\n",
      " - Combined: c:\\Users\\vinay\\Documents\\mnist\\processed_faces\\combined\n",
      " - Results: c:\\Users\\vinay\\Documents\\mnist\\output\n",
      " - Checkpoints: c:\\Users\\vinay\\Documents\\mnist\\output\\checkpoints\n",
      " - Logs: c:\\Users\\vinay\\Documents\\mnist\\output\\logs\n",
      "CelebDF dataset: Found at c:\\Users\\vinay\\Documents\\mnist\\faces\\Real\\Celeb_V2\\Train\\real\n",
      "FaceForensics++ dataset: Found at c:\\Users\\vinay\\Documents\\mnist\\faces\\Real\\FaceForensics++\\original_sequences\\youtube\\c23\\frames\n",
      "Configuration saved to c:\\Users\\vinay\\Documents\\mnist\\output\\face_gan_20250306_113635_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\vinay\\\\Documents\\\\mnist\\\\output\\\\face_gan_20250306_113635_config.json'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################################################\n",
    "# CELL 3: HELPER FUNCTIONS\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Helper Functions\n",
    "--------------\n",
    "Create output directories, verify dataset paths,\n",
    "save configuration, and monitor system resources.\n",
    "\"\"\"\n",
    "def create_output_directories():\n",
    "    \"\"\"Create necessary output directories\"\"\"\n",
    "    os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    os.makedirs(LOG_DIR, exist_ok=True)\n",
    "    \n",
    "    # Create separate directories for each dataset\n",
    "    celebdf_dir = os.path.join(PROCESSED_PATH, \"celebdf\")\n",
    "    ff_dir = os.path.join(PROCESSED_PATH, \"faceforensics\")\n",
    "    combined_dir = os.path.join(PROCESSED_PATH, \"combined\")\n",
    "    \n",
    "    os.makedirs(celebdf_dir, exist_ok=True)\n",
    "    os.makedirs(ff_dir, exist_ok=True)\n",
    "    os.makedirs(combined_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Created output directories:\")\n",
    "    print(f\" - CelebDF: {celebdf_dir}\")\n",
    "    print(f\" - FaceForensics++: {ff_dir}\")\n",
    "    print(f\" - Combined: {combined_dir}\")\n",
    "    print(f\" - Results: {OUTPUT_PATH}\")\n",
    "    print(f\" - Checkpoints: {CHECKPOINT_DIR}\")\n",
    "    print(f\" - Logs: {LOG_DIR}\")\n",
    "    \n",
    "    return celebdf_dir, ff_dir, combined_dir\n",
    "\n",
    "def verify_dataset_paths():\n",
    "    \"\"\"Verify that dataset paths exist\"\"\"\n",
    "    celebdf_exists = os.path.exists(CELEBDF_PATH)\n",
    "    ff_exists = os.path.exists(FF_PATH)\n",
    "    \n",
    "    print(f\"CelebDF dataset: {'Found' if celebdf_exists else 'Not found'} at {CELEBDF_PATH}\")\n",
    "    print(f\"FaceForensics++ dataset: {'Found' if ff_exists else 'Not found'} at {FF_PATH}\")\n",
    "    \n",
    "    return celebdf_exists, ff_exists\n",
    "\n",
    "def save_config():\n",
    "    \"\"\"Save current configuration as a JSON file\"\"\"\n",
    "    config = {\n",
    "        'CELEBDF_PATH': CELEBDF_PATH,\n",
    "        'FF_PATH': FF_PATH,\n",
    "        'PROCESSED_PATH': PROCESSED_PATH,\n",
    "        'OUTPUT_PATH': OUTPUT_PATH,\n",
    "        'CHECKPOINT_DIR': CHECKPOINT_DIR,\n",
    "        'PROCESS_DATASETS': PROCESS_DATASETS,\n",
    "        'CELEBDF_MAX_PER_FACE': CELEBDF_MAX_PER_FACE,\n",
    "        'FF_MAX_PER_FACE': FF_MAX_PER_FACE,\n",
    "        'FF_MAX_FACES': FF_MAX_FACES,\n",
    "        'TARGET_SIZE': TARGET_SIZE,\n",
    "        'BATCH_SIZE': BATCH_SIZE,\n",
    "        'IMAGE_CHANNEL': IMAGE_CHANNEL,\n",
    "        'Z_DIM': Z_DIM,\n",
    "        'G_HIDDEN': G_HIDDEN,\n",
    "        'D_HIDDEN': D_HIDDEN,\n",
    "        'X_DIM': X_DIM,\n",
    "        'EPOCH_NUM': EPOCH_NUM,\n",
    "        'lr': lr,\n",
    "        'seed': seed,\n",
    "        'EXPERIMENT_NAME': EXPERIMENT_NAME,\n",
    "        'timestamp': datetime.datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    config_path = os.path.join(OUTPUT_PATH, f\"{EXPERIMENT_NAME}_config.json\")\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    print(f\"Configuration saved to {config_path}\")\n",
    "    return config_path\n",
    "\n",
    "def monitor_resources():\n",
    "    \"\"\"Monitor system resources and return a report\"\"\"\n",
    "    if not RESOURCE_MONITORING:\n",
    "        return \"Resource monitoring unavailable. Install psutil and gputil.\"\n",
    "    \n",
    "    # CPU info\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    memory = psutil.virtual_memory()\n",
    "    memory_percent = memory.percent\n",
    "    \n",
    "    # GPU info\n",
    "    gpu_info = \"No GPU available\"\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            if gpus:\n",
    "                gpu = gpus[0]  # Get the first GPU\n",
    "                gpu_name = gpu.name\n",
    "                gpu_load = f\"{gpu.load * 100:.1f}%\"\n",
    "                gpu_mem_used = f\"{gpu.memoryUsed:.0f}MB\"\n",
    "                gpu_mem_total = f\"{gpu.memoryTotal:.0f}MB\"\n",
    "                gpu_mem_percent = f\"{(gpu.memoryUsed / gpu.memoryTotal) * 100:.1f}%\"\n",
    "                gpu_temp = f\"{gpu.temperature}Â°C\"\n",
    "                gpu_info = f\"{gpu_name}: {gpu_load} load, {gpu_mem_used}/{gpu_mem_total} ({gpu_mem_percent}), {gpu_temp}\"\n",
    "        except Exception as e:\n",
    "            gpu_info = f\"Error getting GPU info: {e}\"\n",
    "    \n",
    "    return {\n",
    "        \"cpu_percent\": cpu_percent,\n",
    "        \"memory_percent\": memory_percent,\n",
    "        \"gpu_info\": gpu_info,\n",
    "        \"timestamp\": datetime.datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "# Create directories and verify paths\n",
    "celebdf_dir, ff_dir, combined_dir = create_output_directories()\n",
    "celebdf_exists, ff_exists = verify_dataset_paths()\n",
    "save_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping CelebDF processing. Set PROCESS_DATASETS=True to process.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###############################################################################\n",
    "# CELL 4: CELEBDF DATASET PROCESSING\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "CelebDF Dataset Processing\n",
    "------------------------\n",
    "Process the CelebDF dataset by extracting faces, resizing them,\n",
    "and saving them to a standardized format.\n",
    "\"\"\"\n",
    "def process_celebdf_dataset(source_dir, target_dir, target_size=(128, 128), max_images=None):\n",
    "    \"\"\"Process CelebDF dataset where all images are in a single folder\"\"\"\n",
    "    source_path = Path(source_dir)\n",
    "    target_path = Path(target_dir)\n",
    "    \n",
    "    if not source_path.exists():\n",
    "        print(f\"Source directory {source_path} does not exist.\")\n",
    "        return 0\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png']\n",
    "    image_files = []\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(source_path.glob(f\"*{ext}\"))\n",
    "        image_files.extend(source_path.glob(f\"*{ext.upper()}\"))\n",
    "    \n",
    "    # If maximum image limit is set, randomly sample\n",
    "    if max_images and len(image_files) > max_images:\n",
    "        random.seed(1)  # For reproducible results\n",
    "        image_files = random.sample(image_files, max_images)\n",
    "    \n",
    "    print(f\"Found {len(image_files)} total images in {source_path}\")\n",
    "    \n",
    "    # Process all images directly with progress bar\n",
    "    count = 0\n",
    "    with tqdm(total=len(image_files), desc=\"Processing CelebDF images\", unit=\"img\") as pbar:\n",
    "        for img_path in image_files:\n",
    "            try:\n",
    "                # Open and resize image\n",
    "                img = Image.open(img_path)\n",
    "                img = img.resize(target_size, Image.LANCZOS)\n",
    "                \n",
    "                # Save to target directory\n",
    "                target_file = target_path / f\"celebdf_{count:06d}{img_path.suffix}\"\n",
    "                img.save(target_file)\n",
    "                count += 1\n",
    "                pbar.update(1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing {img_path.name}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully processed {count} images from CelebDF to {target_path}\")\n",
    "    return count\n",
    "\n",
    "# Only run if PROCESS_DATASETS is True and CelebDF dataset exists\n",
    "if PROCESS_DATASETS and celebdf_exists:\n",
    "    count = process_celebdf_dataset(\n",
    "        CELEBDF_PATH, \n",
    "        celebdf_dir, \n",
    "        target_size=(TARGET_SIZE, TARGET_SIZE),\n",
    "        max_images=None  # Set to None to process all images, or a number to limit\n",
    "    )\n",
    "    print(f\"Processed {count} CelebDF images\")\n",
    "else:\n",
    "    print(\"Skipping CelebDF processing. Set PROCESS_DATASETS=True to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 5: FACEFORENSICS++ DATASET PROCESSING\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "FaceForensics++ Dataset Processing\n",
    "-------------------------------\n",
    "Process the FaceForensics++ dataset by extracting face folders,\n",
    "resizing images, and saving them to a standardized format.\n",
    "\"\"\"\n",
    "def process_faceforensics_dataset(source_dir, target_dir, target_size=(128, 128), max_folders=None, max_images_per_folder=None):\n",
    "    \"\"\"Process FaceForensics++ dataset where images are organized in numbered folders\"\"\"\n",
    "    source_path = Path(source_dir)\n",
    "    target_path = Path(target_dir)\n",
    "    \n",
    "    if not source_path.exists():\n",
    "        print(f\"Source directory {source_path} does not exist.\")\n",
    "        return 0\n",
    "    \n",
    "    # Get all folders in the source directory\n",
    "    face_folders = [f for f in source_path.iterdir() if f.is_dir()]\n",
    "    \n",
    "    # Limit number of folders if specified\n",
    "    if max_folders and len(face_folders) > max_folders:\n",
    "        face_folders = random.sample(face_folders, max_folders)\n",
    "    \n",
    "    print(f\"Found {len(face_folders)} face folders in {source_path}\")\n",
    "    \n",
    "    # Count total images to process for progress bar\n",
    "    total_images = 0\n",
    "    folder_image_counts = []\n",
    "    for folder in face_folders:\n",
    "        image_files = []\n",
    "        for ext in ['.jpg', '.jpeg', '.png']:\n",
    "            image_files.extend(folder.glob(f\"*{ext}\"))\n",
    "            image_files.extend(folder.glob(f\"*{ext.upper()}\"))\n",
    "        \n",
    "        count = len(image_files)\n",
    "        if max_images_per_folder and count > max_images_per_folder:\n",
    "            count = max_images_per_folder\n",
    "        \n",
    "        folder_image_counts.append(count)\n",
    "        total_images += count\n",
    "    \n",
    "    # Process images in each folder with progress bar\n",
    "    count = 0\n",
    "    with tqdm(total=total_images, desc=\"Processing FaceForensics++ images\", unit=\"img\") as pbar:\n",
    "        for folder_idx, folder in enumerate(face_folders):\n",
    "            # Get image files in this folder\n",
    "            image_files = []\n",
    "            for ext in ['.jpg', '.jpeg', '.png']:\n",
    "                image_files.extend(folder.glob(f\"*{ext}\"))\n",
    "                image_files.extend(folder.glob(f\"*{ext.upper()}\"))\n",
    "            \n",
    "            # Limit number of images per folder if specified\n",
    "            if max_images_per_folder and len(image_files) > max_images_per_folder:\n",
    "                image_files = random.sample(image_files, max_images_per_folder)\n",
    "            \n",
    "                try:\n",
    "                    # Open and resize image\n",
    "                    img = Image.open(img_path)\n",
    "                    img = img.resize(target_size, Image.LANCZOS)\n",
    "                    \n",
    "                    # Save to target directory with folder index as face ID\n",
    "                    target_file = target_path / f\"ff_{folder_idx:04d}_{img_idx:04d}{img_path.suffix}\"\n",
    "                    img.save(target_file)\n",
    "                    count += 1\n",
    "                    pbar.update(1)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing {img_path.name}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully processed {count} images from FaceForensics++ to {target_path}\")\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping FaceForensics++ processing. Set PROCESS_DATASETS=True to process.\n"
     ]
    }
   ],
   "source": [
    "# Only run if PROCESS_DATASETS is True and FaceForensics++ dataset exists\n",
    "if PROCESS_DATASETS and ff_exists:\n",
    "    count = process_faceforensics_dataset(\n",
    "        FF_PATH, \n",
    "        ff_dir, \n",
    "        target_size=(TARGET_SIZE, TARGET_SIZE),\n",
    "        max_folders=FF_MAX_FACES,\n",
    "        max_images_per_folder=FF_MAX_PER_FACE\n",
    "    )\n",
    "    print(f\"Processed {count} FaceForensics++ images\")\n",
    "else:\n",
    "    print(\"Skipping FaceForensics++ processing. Set PROCESS_DATASETS=True to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CelebDF dataset at c:\\Users\\vinay\\Documents\\mnist\\processed_faces\\celebdf\n",
      "Dataset path for training: c:\\Users\\vinay\\Documents\\mnist\\processed_faces\\celebdf\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# CELL 6: COMBINE DATASETS\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Dataset Combination\n",
    "----------------\n",
    "Combine the processed CelebDF and FaceForensics++ datasets \n",
    "into a single dataset for training. This ensures we have a diverse\n",
    "set of facial images.\n",
    "\"\"\"\n",
    "def combine_datasets(celebdf_dir, ff_dir, combined_dir):\n",
    "    \"\"\"Combine processed datasets into one directory\"\"\"\n",
    "    # Copy all images from CelebDF directory\n",
    "    celebdf_files = list(Path(celebdf_dir).glob(\"*.jpg\")) + list(Path(celebdf_dir).glob(\"*.png\"))\n",
    "    ff_files = list(Path(ff_dir).glob(\"*.jpg\")) + list(Path(ff_dir).glob(\"*.png\"))\n",
    "    \n",
    "    total_files = len(celebdf_files) + len(ff_files)\n",
    "    print(f\"Combining {len(celebdf_files)} CelebDF images and {len(ff_files)} FaceForensics++ images...\")\n",
    "    \n",
    "    with tqdm(total=total_files, desc=\"Combining datasets\", unit=\"img\") as pbar:\n",
    "        # Copy CelebDF images\n",
    "        for img_path in celebdf_files:\n",
    "            try:\n",
    "                target_path = Path(combined_dir) / img_path.name\n",
    "                img = Image.open(img_path)\n",
    "                img.save(target_path)\n",
    "                pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError copying {img_path.name}: {e}\")\n",
    "        \n",
    "        # Copy FaceForensics++ images\n",
    "        for img_path in ff_files:\n",
    "            try:\n",
    "                target_path = Path(combined_dir) / img_path.name\n",
    "                img = Image.open(img_path)\n",
    "                img.save(target_path)\n",
    "                pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError copying {img_path.name}: {e}\")\n",
    "    \n",
    "    print(f\"Combined dataset created with {total_files} total images at {combined_dir}\")\n",
    "    return total_files\n",
    "\n",
    "# Only run if both datasets have been processed\n",
    "if PROCESS_DATASETS and celebdf_exists and ff_exists:\n",
    "    total_images = combine_datasets(celebdf_dir, ff_dir, combined_dir)\n",
    "    dataset_path = combined_dir\n",
    "    print(f\"Combined dataset created with {total_images} images\")\n",
    "elif celebdf_exists and os.path.exists(celebdf_dir) and len(os.listdir(celebdf_dir)) > 0:\n",
    "    dataset_path = celebdf_dir\n",
    "    print(f\"Using CelebDF dataset at {celebdf_dir}\")\n",
    "elif ff_exists and os.path.exists(ff_dir) and len(os.listdir(ff_dir)) > 0:\n",
    "    dataset_path = ff_dir\n",
    "    print(f\"Using FaceForensics++ dataset at {ff_dir}\")\n",
    "elif os.path.exists(combined_dir) and len(os.listdir(combined_dir)) > 0:\n",
    "    dataset_path = combined_dir\n",
    "    print(f\"Using existing combined dataset at {combined_dir}\")\n",
    "else:\n",
    "    print(\"Error: No datasets were processed or found\")\n",
    "    dataset_path = None\n",
    "\n",
    "print(f\"Dataset path for training: {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu126\n",
      "CUDA version: 12.6\n",
      "GPU: NVIDIA GeForce RTX 2070 Super with Max-Q Design\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###############################################################################\n",
    "# CELL 7: DATASET CLASS AND CUDA SETUP\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Dataset Class and CUDA Setup\n",
    "-------------------------\n",
    "Define the custom dataset class for loading face images\n",
    "and set up CUDA for GPU acceleration if available.\n",
    "\"\"\"\n",
    "class FaceDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading face images\"\"\"\n",
    "    def __init__(self, root_dir, transform=None, file_extensions=('.png', '.jpg', '.jpeg')):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.file_extensions = file_extensions\n",
    "        \n",
    "        # Count all files first for debugging\n",
    "        all_files = os.listdir(root_dir)\n",
    "        print(f\"Total files in directory: {len(all_files)}\")\n",
    "        \n",
    "        # Get valid image files\n",
    "        self.image_files = [\n",
    "            f for f in all_files \n",
    "            if os.path.isfile(os.path.join(root_dir, f)) and \n",
    "               any(f.lower().endswith(ext) for ext in self.file_extensions)\n",
    "        ]\n",
    "        \n",
    "        # Log file extension stats\n",
    "        extensions = {}\n",
    "        for f in self.image_files:\n",
    "            ext = os.path.splitext(f)[1].lower()\n",
    "            extensions[ext] = extensions.get(ext, 0) + 1\n",
    "        \n",
    "        print(f\"Found {len(self.image_files)} valid images out of {len(all_files)} total files\")\n",
    "        print(f\"Image extension breakdown: {extensions}\")\n",
    "        \n",
    "        # Check for potentially problematic files\n",
    "        non_image_files = len(all_files) - len(self.image_files)\n",
    "        if non_image_files > 0:\n",
    "            print(f\"Warning: {non_image_files} files were skipped (not recognized as images)\")\n",
    "            # Sample some skipped files for debugging\n",
    "            skipped = [f for f in all_files if f not in self.image_files][:5]\n",
    "            print(f\"Sample skipped files: {skipped}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, 0  # Return 0 as dummy label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            # Return a placeholder black image if loading fails\n",
    "            if self.transform:\n",
    "                return torch.zeros(3, 128, 128), 0\n",
    "            else:\n",
    "                return Image.new('RGB', (128, 128), color='black'), 0\n",
    "\n",
    "def setup_cuda():\n",
    "    \"\"\"Setup CUDA if available\"\"\"\n",
    "    global CUDA, device\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        CUDA = False\n",
    "        print(\"CUDA is not available. Running on CPU.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "    else:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        # Enable deterministic behavior for reproducibility\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        cudnn.benchmark = True\n",
    "    \n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    if CUDA:\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device\n",
    "\n",
    "# Setup CUDA\n",
    "device = setup_cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator initialized\n",
      "Discriminator initialized\n",
      "\n",
      "Generator Architecture:\n",
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (12): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (15): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (16): Tanh()\n",
      "  )\n",
      ")\n",
      "\n",
      "Discriminator Architecture:\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (12): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (14): Conv2d(1024, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (15): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# CELL 8: GAN MODEL ARCHITECTURE\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "GAN Model Architecture\n",
    "-------------------\n",
    "Define the Generator and Discriminator architecture\n",
    "for the Deep Convolutional GAN (DCGAN).\n",
    "\"\"\"\n",
    "def weights_init(m):\n",
    "    \"\"\"Initialize network weights\"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "# Replace your current Generator class in Cell 8 with this code:\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator Network with LeakyReLU for better gradient flow\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: Z_DIM x 1 x 1\n",
    "            nn.ConvTranspose2d(Z_DIM, G_HIDDEN * 16, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # Changed from ReLU to LeakyReLU\n",
    "            # 4x4\n",
    "            \n",
    "            nn.ConvTranspose2d(G_HIDDEN * 16, G_HIDDEN * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # Changed from ReLU to LeakyReLU\n",
    "            # 8x8\n",
    "            \n",
    "            nn.ConvTranspose2d(G_HIDDEN * 8, G_HIDDEN * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # Changed from ReLU to LeakyReLU\n",
    "            # 16x16\n",
    "            \n",
    "            nn.ConvTranspose2d(G_HIDDEN * 4, G_HIDDEN * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # Changed from ReLU to LeakyReLU\n",
    "            # 32x32\n",
    "            \n",
    "            nn.ConvTranspose2d(G_HIDDEN * 2, G_HIDDEN, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # Changed from ReLU to LeakyReLU\n",
    "            # 64x64\n",
    "            \n",
    "            nn.ConvTranspose2d(G_HIDDEN, IMAGE_CHANNEL, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # 128x128\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator Network\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: IMAGE_CHANNEL x 128 x 128\n",
    "            nn.Conv2d(IMAGE_CHANNEL, D_HIDDEN, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 64x64\n",
    "            \n",
    "            nn.Conv2d(D_HIDDEN, D_HIDDEN * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 32x32\n",
    "            \n",
    "            nn.Conv2d(D_HIDDEN * 2, D_HIDDEN * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 16x16\n",
    "            \n",
    "            nn.Conv2d(D_HIDDEN * 4, D_HIDDEN * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 8x8\n",
    "            \n",
    "            nn.Conv2d(D_HIDDEN * 8, D_HIDDEN * 16, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 4x4\n",
    "            \n",
    "            nn.Conv2d(D_HIDDEN * 16, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # 1x1\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input).view(-1, 1).squeeze(1)\n",
    "\n",
    "# Initialize networks\n",
    "netG = Generator().to(device)\n",
    "netG.apply(weights_init)\n",
    "print(\"Generator initialized\")\n",
    "\n",
    "netD = Discriminator().to(device)\n",
    "netD.apply(weights_init)\n",
    "print(\"Discriminator initialized\")\n",
    "\n",
    "# Display network architecture summaries\n",
    "print(\"\\nGenerator Architecture:\")\n",
    "print(netG)\n",
    "print(\"\\nDiscriminator Architecture:\")\n",
    "print(netD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset comparison: Current path has 80576 images, Combined directory has 144473 images\n",
      "Switching to combined dataset with more images: c:\\Users\\vinay\\Documents\\mnist\\processed_faces\\combined\n",
      "Looking for images in: c:\\Users\\vinay\\Documents\\mnist\\processed_faces\\combined\n",
      "Total files in directory: 144473\n",
      "Found 144473 valid images out of 144473 total files\n",
      "Image extension breakdown: {'.jpg': 80576, '.png': 63897}\n",
      "Dataset loaded with 144473 images\n",
      "Number of batches: 2258\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###############################################################################\n",
    "# CELL 9: DATA LOADING WITH ENHANCED DIAGNOSTICS (VISUALIZATION REMOVED)\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Data Loading with Enhanced Diagnostics\n",
    "---------------------------\n",
    "Load the processed face dataset with better diagnostics and file handling.\n",
    "Includes checks for the combined directory and detailed file reporting.\n",
    "Visualization has been removed to improve performance.\n",
    "\"\"\"\n",
    "def load_dataset(data_path):\n",
    "    \"\"\"Load face dataset and create dataloader with detailed diagnostics\"\"\"\n",
    "    # Image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(X_DIM),\n",
    "        transforms.CenterCrop(X_DIM),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize for RGB\n",
    "    ])\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    try:\n",
    "        print(f\"Looking for images in: {data_path}\")\n",
    "        \n",
    "        # Check if the directory exists and is readable\n",
    "        if not os.path.exists(data_path):\n",
    "            print(f\"Error: Directory {data_path} does not exist\")\n",
    "            return None, None\n",
    "        elif not os.access(data_path, os.R_OK):\n",
    "            print(f\"Error: No read permissions for {data_path}\")\n",
    "            return None, None\n",
    "            \n",
    "        # Check if path is combined dir and has more images\n",
    "        combined_dir = os.path.join(PROCESSED_PATH, \"combined\")\n",
    "        if os.path.exists(combined_dir) and data_path != combined_dir:\n",
    "            combined_count = len([f for f in os.listdir(combined_dir) \n",
    "                               if os.path.isfile(os.path.join(combined_dir, f)) and \n",
    "                               f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "            current_count = len([f for f in os.listdir(data_path) \n",
    "                               if os.path.isfile(os.path.join(data_path, f)) and \n",
    "                               f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "            \n",
    "            if combined_count > current_count:\n",
    "                print(f\"Note: The combined directory has {combined_count} images, but you're using {data_path} with {current_count} images\")\n",
    "                print(f\"Consider using the combined dataset at {combined_dir} instead\")\n",
    "                \n",
    "                # Optionally, switch to the combined directory automatically\n",
    "                # Uncomment the line below to use the combined directory with more images\n",
    "                # data_path = combined_dir\n",
    "                \n",
    "        # Instantiate the dataset with additional file extensions\n",
    "        dataset = FaceDataset(\n",
    "            root_dir=data_path, \n",
    "            transform=transform,\n",
    "            file_extensions=('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.webp')  # Add more image types\n",
    "        )\n",
    "        \n",
    "        if len(dataset) == 0:\n",
    "            print(f\"Error: No valid images found in {data_path}\")\n",
    "            return None, None\n",
    "            \n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=True,\n",
    "            num_workers=0, \n",
    "            pin_memory=True if CUDA else False\n",
    "        )\n",
    "        print(f\"Dataset loaded with {len(dataset)} images\")\n",
    "        print(f\"Number of batches: {len(dataloader)}\")\n",
    "        return dataset, dataloader\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Load dataset if path exists\n",
    "if dataset_path and os.path.exists(dataset_path):\n",
    "    # Check if combined dataset exists and has more images than current dataset\n",
    "    combined_dir = os.path.join(PROCESSED_PATH, \"combined\")\n",
    "    if os.path.exists(combined_dir):\n",
    "        # Quick count of valid images in both directories\n",
    "        combined_files = [f for f in os.listdir(combined_dir) \n",
    "                         if os.path.isfile(os.path.join(combined_dir, f)) and \n",
    "                         f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        current_files = [f for f in os.listdir(dataset_path) \n",
    "                        if os.path.isfile(os.path.join(dataset_path, f)) and \n",
    "                        f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        print(f\"Dataset comparison: Current path has {len(current_files)} images, Combined directory has {len(combined_files)} images\")\n",
    "        \n",
    "        if len(combined_files) > len(current_files):\n",
    "            print(f\"Switching to combined dataset with more images: {combined_dir}\")\n",
    "            dataset_path = combined_dir\n",
    "    \n",
    "    dataset, dataloader = load_dataset(dataset_path)\n",
    "    if not dataset or not dataloader:\n",
    "        print(\"Failed to load dataset or create dataloader.\")\n",
    "else:\n",
    "    print(\"Dataset path not available. Please check your configurations.\")\n",
    "    dataloader = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 10: CHECKPOINT FUNCTIONS\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Checkpoint Functions\n",
    "----------------\n",
    "Functions for saving and loading model checkpoints\n",
    "to support resumable training and best model selection.\n",
    "\"\"\"\n",
    "def save_checkpoint(netG, netD, optimG, optimD, epoch, iteration, losses, img_list, filename=None):\n",
    "    \"\"\"Save a checkpoint of the current training state\"\"\"\n",
    "    if not filename:\n",
    "        filename = os.path.join(CHECKPOINT_DIR, f\"checkpoint_e{epoch}_i{iteration}.pt\")\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'iteration': iteration,\n",
    "        'netG_state_dict': netG.state_dict(),\n",
    "        'netD_state_dict': netD.state_dict(),\n",
    "        'optimG_state_dict': optimG.state_dict(),\n",
    "        'optimD_state_dict': optimD.state_dict(),\n",
    "        'G_losses': losses['G'],\n",
    "        'D_losses': losses['D'],\n",
    "        'img_list': img_list,\n",
    "        'timestamp': datetime.datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved: {filename}\")\n",
    "    \n",
    "    # Save the latest checkpoint also (for resuming)\n",
    "    latest_path = os.path.join(CHECKPOINT_DIR, \"latest_checkpoint.pt\")\n",
    "    torch.save(checkpoint, latest_path)\n",
    "    \n",
    "    return filename\n",
    "\n",
    "def load_checkpoint(netG, netD, optimG, optimD, filename=None):\n",
    "    \"\"\"Load a checkpoint to resume training\"\"\"\n",
    "    if not filename:\n",
    "        # Try to load the latest checkpoint\n",
    "        filename = os.path.join(CHECKPOINT_DIR, \"latest_checkpoint.pt\")\n",
    "    \n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"No checkpoint found at {filename}\")\n",
    "        return None, 0, 0, {'G': [], 'D': []}, []\n",
    "    \n",
    "    print(f\"Loading checkpoint: {filename}\")\n",
    "    checkpoint = torch.load(filename, map_location=device)\n",
    "    \n",
    "    netG.load_state_dict(checkpoint['netG_state_dict'])\n",
    "    netD.load_state_dict(checkpoint['netD_state_dict'])\n",
    "    optimG.load_state_dict(checkpoint['optimG_state_dict'])\n",
    "    optimD.load_state_dict(checkpoint['optimD_state_dict'])\n",
    "    \n",
    "    # For optimizers loaded from checkpoint, move to correct device\n",
    "    if CUDA:\n",
    "        for state in optimG.state.values():\n",
    "            for k, v in state.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    state[k] = v.to(device)\n",
    "                    \n",
    "        for state in optimD.state.values():\n",
    "            for k, v in state.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    state[k] = v.to(device)\n",
    "    \n",
    "    epoch = checkpoint['epoch']\n",
    "    iteration = checkpoint['iteration']\n",
    "    G_losses = checkpoint['G_losses']\n",
    "    D_losses = checkpoint['D_losses']\n",
    "    img_list = checkpoint.get('img_list', [])\n",
    "    \n",
    "    print(f\"Resuming from epoch {epoch+1}, iteration {iteration}\")\n",
    "    \n",
    "    return {'netG': netG, 'netD': netD, 'optimG': optimG, 'optimD': optimD}, epoch, iteration, {'G': G_losses, 'D': D_losses}, img_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found at c:\\Users\\vinay\\Documents\\mnist\\output\\checkpoints\\latest_checkpoint.pt\n",
      "\n",
      "==================================================\n",
      "READY FOR TRAINING\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# CELL 11: TRAINING INITIALIZATION\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Training Initialization\n",
    "--------------------\n",
    "Initialize GAN training by setting up optimizers, \n",
    "creating fixed noise for visualization,\n",
    "and loading a checkpoint if resuming training.\n",
    "\"\"\"\n",
    "# Setup loss function and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "# Fixed noise for visualization\n",
    "fixed_noise = torch.randn(16, Z_DIM, 1, 1, device=device)\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Setup TensorBoard\n",
    "if ENABLE_TENSORBOARD:\n",
    "    tb_writer = SummaryWriter(log_dir=os.path.join(LOG_DIR, EXPERIMENT_NAME))\n",
    "    # Add model graph to TensorBoard\n",
    "    try:\n",
    "        sample_input = torch.randn(1, Z_DIM, 1, 1, device=device)\n",
    "        tb_writer.add_graph(netG, sample_input)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not add model graph to TensorBoard: {e}\")\n",
    "else:\n",
    "    tb_writer = None\n",
    "\n",
    "# Try to load checkpoint if resuming training\n",
    "start_epoch = 0\n",
    "start_iter = 0\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "img_list = []\n",
    "\n",
    "if RESUME_TRAINING:\n",
    "    models, start_epoch, start_iter, losses, prev_img_list = load_checkpoint(netG, netD, optimizerG, optimizerD)\n",
    "    if models:  # If checkpoint was loaded successfully\n",
    "        netG, netD = models['netG'], models['netD']\n",
    "        optimizerG, optimizerD = models['optimG'], models['optimD']\n",
    "        G_losses = losses['G']\n",
    "        D_losses = losses['D']\n",
    "        img_list = prev_img_list\n",
    "        print(f\"Resuming from epoch {start_epoch+1}, iteration {start_iter}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\\nREADY FOR TRAINING\\n{'='*50}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output directory structure at c:\\Users\\vinay\\Documents\\mnist\\output\\face_gan_run_20250306_113703\n",
      "Total iterations: 22570\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a58952a6974d62aa6dc1875f5d068e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Total progress:   0%|          | 0/22570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e218e01e50be42e688ec2bee67e28489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/2257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "###############################################################################\n",
    "# CELL 12: ENHANCED TRAINING LOOP WITH STABILIZATION TECHNIQUES\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Enhanced Training Loop for Face GAN\n",
    "----------------------------------\n",
    "Features:\n",
    "- Label smoothing (real_label=0.9)\n",
    "- Adaptive learning rates\n",
    "- Noisy label training\n",
    "- Conditional discriminator updates\n",
    "- Two-timescale update rule (TTUR)\n",
    "- Multiple generator updates for difficult batches\n",
    "\"\"\"\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "\n",
    "# Create output directory structure\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_name = f\"face_gan_run_{timestamp}\"\n",
    "run_dir = os.path.join(OUTPUT_PATH, run_name)\n",
    "\n",
    "# Create subdirectories for all outputs\n",
    "images_dir = os.path.join(run_dir, \"images\")\n",
    "#checkpoints_dir = os.path.join(run_dir, \"checkpoints\") this was a bug.\n",
    "checkpoints_dir = CHECKPOINT_DIR \n",
    "logs_dir = os.path.join(run_dir, \"logs\")\n",
    "timelapse_dir = os.path.join(run_dir, \"timelapse\")\n",
    "\n",
    "# Create timelapse subdirectories\n",
    "angles = [\"grid\", \"row\", \"individual\", \"interpolation\"]\n",
    "timelapse_subdirs = {}\n",
    "for angle in angles:\n",
    "    dir_path = os.path.join(timelapse_dir, angle)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    timelapse_subdirs[angle] = dir_path\n",
    "\n",
    "# Create all other directories\n",
    "for dir_path in [run_dir, images_dir, checkpoints_dir, logs_dir]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(f\"Created output directory structure at {run_dir}\")\n",
    "\n",
    "# GPU Optimization Steps\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if CUDA else False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Calculate total iterations\n",
    "total_iterations = len(dataloader) * EPOCH_NUM\n",
    "print(f\"Total iterations: {total_iterations}\")\n",
    "\n",
    "# Setup CSV logging file\n",
    "csv_log_path = os.path.join(logs_dir, \"training_metrics.csv\")\n",
    "with open(csv_log_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Epoch', 'Iteration', 'G_loss', 'D_loss', 'D_x', 'D_G_z1', 'D_G_z2', \n",
    "                     'Time_elapsed', 'ETA'])\n",
    "\n",
    "# Create fixed noise vectors for visualization\n",
    "fixed_noise = torch.randn(25, Z_DIM, 1, 1, device=device)\n",
    "fixed_noise_row = torch.randn(8, Z_DIM, 1, 1, device=device)\n",
    "interpolation_points = 10\n",
    "fixed_noise_start = torch.randn(1, Z_DIM, 1, 1, device=device)\n",
    "fixed_noise_end = torch.randn(1, Z_DIM, 1, 1, device=device)\n",
    "interpolation_noises = []\n",
    "for i in range(interpolation_points):\n",
    "    t = i / (interpolation_points - 1)\n",
    "    interp_noise = (1-t) * fixed_noise_start + t * fixed_noise_end\n",
    "    interpolation_noises.append(interp_noise)\n",
    "\n",
    "\n",
    "# Modified save_timelapse_images function to use fixed seeds for individual images\n",
    "def save_timelapse_images(netG, epoch, iteration, total_iterations):\n",
    "    filename_base = f\"{epoch:03d}_{iteration:05d}\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Grid of images - use the existing fixed_noise\n",
    "        fake_grid = netG(fixed_noise).detach().cpu()\n",
    "        grid_img = vutils.make_grid(fake_grid, padding=2, normalize=True, nrow=5)\n",
    "        utils_filename = os.path.join(timelapse_subdirs[\"grid\"], f\"grid_{filename_base}.png\") \n",
    "        vutils.save_image(fake_grid, utils_filename, nrow=5, padding=2, normalize=True)\n",
    "        \n",
    "        # Row of images - use the existing fixed_noise_row\n",
    "        fake_row = netG(fixed_noise_row).detach().cpu()\n",
    "        row_filename = os.path.join(timelapse_subdirs[\"row\"], f\"row_{filename_base}.png\")\n",
    "        vutils.save_image(fake_row, row_filename, nrow=8, padding=2, normalize=True)\n",
    "        \n",
    "        # Individual large images - use individual_fixed_noises\n",
    "        # We'll use the first 4 samples from our fixed_noise to maintain consistency\n",
    "        indiv_samples = netG(fixed_noise[:4]).detach().cpu()\n",
    "        for i, sample in enumerate(indiv_samples):\n",
    "            indiv_filename = os.path.join(timelapse_subdirs[\"individual\"], f\"img{i}_{filename_base}.png\")\n",
    "            vutils.save_image(sample, indiv_filename, normalize=True)\n",
    "        \n",
    "        # Latent space interpolation - keep the existing approach\n",
    "        interp_images = []\n",
    "        for noise in interpolation_noises:\n",
    "            interp_images.append(netG(noise).detach().cpu())\n",
    "        interp_tensor = torch.cat(interp_images, 0)\n",
    "        interp_filename = os.path.join(timelapse_subdirs[\"interpolation\"], f\"interp_{filename_base}.png\")\n",
    "        vutils.save_image(interp_tensor, interp_filename, nrow=interpolation_points, normalize=True)\n",
    "    \n",
    "    return grid_img\n",
    "\n",
    "\n",
    "# Function to add noise to labels\n",
    "def noisy_labels(size, value, device):\n",
    "    \"\"\"Add noise to labels to prevent discriminator overconfidence\"\"\"\n",
    "    if value > 0.5:  # real label\n",
    "        # Real labels: value-0.2 to value range instead of exactly value (0.9)\n",
    "        return value - 0.2 * torch.rand(size, device=device)\n",
    "    else:  # fake label\n",
    "        # Fake labels: 0 to 0.3 range instead of exactly 0\n",
    "        return value + 0.3 * torch.rand(size, device=device)\n",
    "\n",
    "# Lists to track progress\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "D_x_history = []  # Real accuracy\n",
    "D_G_z_history = []  # Fake accuracy\n",
    "img_list = []\n",
    "\n",
    "# Training frequency settings\n",
    "record_freq = 100\n",
    "save_image_freq = 200\n",
    "checkpoint_freq = 1\n",
    "\n",
    "# Initialize optimizers with different learning rates (TTUR)\n",
    "lr_g = lr * 0.5  # Lower learning rate for generator (1e-4)\n",
    "lr_d = lr * 1.5  # Higher learning rate for discriminator (3e-4)\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "\n",
    "# Set up labels for training\n",
    "real_label = 0.9  # Use label smoothing\n",
    "fake_label = 0.0\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "global_step = 0\n",
    "\n",
    "# Create progress bars\n",
    "main_progress = tqdm(total=total_iterations, desc=\"Total progress\")\n",
    "\n",
    "try:\n",
    "    # For each epoch\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # TQDM bar for this epoch\n",
    "        pbar = tqdm(total=len(dataloader), desc=f\"Epoch {epoch+1}/{EPOCH_NUM}\")\n",
    "        \n",
    "        # For each batch in the dataloader\n",
    "        for i, data in enumerate(dataloader):\n",
    "            # Calculate ETA and elapsed time\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if global_step > 0:\n",
    "                iterations_left = total_iterations - global_step\n",
    "                eta_seconds = elapsed_time * (iterations_left / global_step)\n",
    "                eta = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "            else:\n",
    "                eta = \"N/A\"\n",
    "                \n",
    "            # Move batch data to device\n",
    "            real_images = data[0].to(device)\n",
    "            current_batch_size = real_images.size(0)\n",
    "            \n",
    "            # --------------------\n",
    "            # Train Discriminator\n",
    "            # --------------------\n",
    "            netD.zero_grad()\n",
    "            \n",
    "            # Real images with noisy labels\n",
    "            real_label_values = noisy_labels(current_batch_size, real_label, device)\n",
    "            output = netD(real_images)\n",
    "            errD_real = criterion(output, real_label_values)\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "            \n",
    "            # Fake images with noisy labels\n",
    "            noise = torch.randn(current_batch_size, Z_DIM, 1, 1, device=device)\n",
    "            fake = netG(noise)\n",
    "            fake_label_values = noisy_labels(current_batch_size, fake_label, device)\n",
    "            output = netD(fake.detach())\n",
    "            errD_fake = criterion(output, fake_label_values)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            \n",
    "            errD = errD_real + errD_fake\n",
    "            \n",
    "            # Only update discriminator if it's not too powerful\n",
    "            if D_x < 0.8 or D_G_z1 > 0.1:\n",
    "                optimizerD.step()\n",
    "\n",
    "            # --------------------\n",
    "            # Train Generator\n",
    "            # --------------------\n",
    "            # Train generator multiple times if it's struggling\n",
    "            generator_updates = 2 if errD.item() < 1.0 and D_G_z1 < 0.3 else 1\n",
    "            \n",
    "            for _ in range(generator_updates):\n",
    "                netG.zero_grad()\n",
    "                label = torch.full((current_batch_size,), real_label, dtype=torch.float, device=device)\n",
    "                \n",
    "                # If we're doing multiple updates, generate fresh noise for second update\n",
    "                if _ > 0:\n",
    "                    noise = torch.randn(current_batch_size, Z_DIM, 1, 1, device=device)\n",
    "                    fake = netG(noise)\n",
    "                    \n",
    "                output = netD(fake)\n",
    "                errG = criterion(output, label)\n",
    "                errG.backward()\n",
    "                optimizerG.step()\n",
    "                \n",
    "            # Get final D(G(z)) after G update\n",
    "            D_G_z2 = output.mean().item()\n",
    "            \n",
    "            # Save losses for plotting later\n",
    "            G_losses.append(errG.item())\n",
    "            D_losses.append(errD.item())\n",
    "            D_x_history.append(D_x)\n",
    "            D_G_z_history.append(D_G_z2)\n",
    "            \n",
    "            # Update TQDM with critical statistics\n",
    "            pbar.set_postfix({\n",
    "                'G_loss': f\"{errG.item():.3f}\",\n",
    "                'D_loss': f\"{errD.item():.3f}\", \n",
    "                'D(x)': f\"{D_x:.3f}\",\n",
    "                'D(G(z))': f\"{D_G_z2:.3f}\"\n",
    "            })\n",
    "            pbar.update(1)\n",
    "            main_progress.update(1)\n",
    "            \n",
    "            # Record metrics to CSV log every record_freq batches\n",
    "            if global_step % record_freq == 0:\n",
    "                with open(csv_log_path, 'a', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    writer.writerow([\n",
    "                        epoch+1, i, errG.item(), errD.item(), \n",
    "                        D_x, D_G_z1, D_G_z2, \n",
    "                        elapsed_time, eta\n",
    "                    ])\n",
    "            \n",
    "            # Generate and save images for timelapse\n",
    "            if global_step % save_image_freq == 0 or (epoch == EPOCH_NUM-1 and i == len(dataloader)-1):\n",
    "                img_grid = save_timelapse_images(netG, epoch+1, global_step, total_iterations)\n",
    "                # Store grid for final evaluation (but don't display it)\n",
    "                img_list.append(img_grid)\n",
    "                \n",
    "                # Save loss curves without displaying\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.title(\"Generator and Discriminator Loss\")\n",
    "                plt.plot(G_losses, label=\"G\")\n",
    "                plt.plot(D_losses, label=\"D\")\n",
    "                plt.legend()\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.title(\"D(x) and D(G(z))\")\n",
    "                plt.plot(D_x_history, label=\"D(x)\")\n",
    "                plt.plot(D_G_z_history, label=\"D(G(z))\")\n",
    "                plt.legend()\n",
    "                plt.savefig(os.path.join(logs_dir, f\"loss_curves_{epoch+1}_{global_step}.png\"))\n",
    "                plt.close()\n",
    "                \n",
    "            # Increment global step counter\n",
    "            global_step += 1\n",
    "                \n",
    "        # Close progress bar for this epoch\n",
    "        pbar.close()\n",
    "        \n",
    "        # End of epoch stats\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1}/{EPOCH_NUM} completed in {epoch_time:.2f}s\")\n",
    "        \n",
    "        # Save checkpoint at end of epoch\n",
    "        if (epoch+1) % checkpoint_freq == 0:\n",
    "            latest_path = os.path.join(CHECKPOINT_DIR, \"latest_checkpoint.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "                'netG_state_dict': netG.state_dict(),\n",
    "                'netD_state_dict': netD.state_dict(),\n",
    "                'optimizerG_state_dict': optimizerG.state_dict(),\n",
    "                'optimizerD_state_dict': optimizerD.state_dict(),\n",
    "                'G_losses': G_losses,\n",
    "                'D_losses': D_losses,\n",
    "                'D_x_history': D_x_history,\n",
    "                'D_G_z_history': D_G_z_history\n",
    "            }, latest_path)\n",
    "            print(f\"Checkpoint saved: {latest_path}\")\n",
    "            '''\n",
    "            checkpoint_path = os.path.join(checkpoints_dir, f\"checkpoint_epoch{epoch+1}.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "                'netG_state_dict': netG.state_dict(),\n",
    "                'netD_state_dict': netD.state_dict(),\n",
    "                'optimizerG_state_dict': optimizerG.state_dict(),\n",
    "                'optimizerD_state_dict': optimizerD.state_dict(),\n",
    "                'G_losses': G_losses,\n",
    "                'D_losses': D_losses,\n",
    "                'D_x_history': D_x_history,\n",
    "                'D_G_z_history': D_G_z_history\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved: {checkpoint_path}\")'''\n",
    "    \n",
    "    # Close main progress bar\n",
    "    main_progress.close()\n",
    "    \n",
    "    # Training complete\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Training completed in {total_time/60:.2f} minutes\")\n",
    "    \n",
    "    # Save final summary statistics\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    # Loss plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(G_losses, label=\"Generator\")\n",
    "    plt.plot(D_losses, label=\"Discriminator\")\n",
    "    plt.title(\"Loss over Training\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # D(x)/D(G(z)) plot\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(D_x_history, label=\"D(x) - Real\")\n",
    "    plt.plot(D_G_z_history, label=\"D(G(z)) - Fake\")\n",
    "    plt.title(\"Discriminator Accuracy\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Final generated images grid\n",
    "    if len(img_list) > 0:\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Final Generated Images\")\n",
    "        plt.imshow(np.transpose(img_list[-1], (1, 2, 0)))\n",
    "    \n",
    "    plt.savefig(os.path.join(run_dir, \"final_summary.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(netG.state_dict(), os.path.join(run_dir, \"generator_final.pt\"))\n",
    "    torch.save(netD.state_dict(), os.path.join(run_dir, \"discriminator_final.pt\"))\n",
    "    \n",
    "    print(f\"Training data saved to {run_dir}\")\n",
    "    print(\"Training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    main_progress.close()\n",
    "    print(f\"Error during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Try to save emergency checkpoint\n",
    "    try:\n",
    "        error_checkpoint_path = os.path.join(checkpoints_dir, \"error_checkpoint.pt\")\n",
    "        torch.save({\n",
    "            'netG_state_dict': netG.state_dict(),\n",
    "            'netD_state_dict': netD.state_dict(),\n",
    "            'optimizerG_state_dict': optimizerG.state_dict(),\n",
    "            'optimizerD_state_dict': optimizerD.state_dict(),\n",
    "            'G_losses': G_losses,\n",
    "            'D_losses': D_losses,\n",
    "            'epoch': epoch,\n",
    "            'global_step': global_step\n",
    "        }, error_checkpoint_path)\n",
    "        print(f\"Emergency checkpoint saved to {error_checkpoint_path}\")\n",
    "    except:\n",
    "        print(\"Failed to save emergency checkpoint\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
